{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>COE49412 Neural Networks and Deep Learning</h2></center>\n",
    "\n",
    "<center><h3>Lab 6 - Implementing Back Propagation</h3></center>\n",
    "<br>\n",
    "<b> Objectives: </b>\n",
    "\n",
    "* To implement a neural network in Python.\n",
    "* To do a single iteration of a forward pass.\n",
    "\n",
    "<b> Refer to your previous lecture notes and notebook (Implementing BP) to solve the exercises </b>\n",
    "\n",
    "<b> Submit: </b>\n",
    "* Jupyter notebook with the solutuion\n",
    "\n",
    "<b> Due Date: Wednesday, 25th March 2020, 11:59pm </b><br><br>\n",
    "<b><i>Please enter your Student ID & Name below:</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student ID: b00069350\n",
    "## Student Name: Danayal Khan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the following exercises, implement the neural networks following the algorithms discussed in Implementing BP lecture.\n",
    "## https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**\n",
    "\n",
    "Consider the simple 3-layer neural network in Figure 1. Implement this neural network and print the predicted values of <b> $z_{i}$ and $a^l$ </b>, after the first forward pass <br>\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex1.png\" style=\"width:400px;\"></center><caption><center><b>Figure 1</b>: 3-layer neural network.</center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- z= self.weights[i].dot(a) + self.biases[i]\n",
    "- a = activation_function(z_s[-1])\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 2$ for the given input $x_{1}$ = 4\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.65 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.75 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing backpropagation\n",
    "## Source: \n",
    "## https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "## comments added to the original code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    # The constructor takes \n",
    "    # 1. layers representing the number of nodes\n",
    "    # 2. activations representing the activation functions to choose in\n",
    "    #    each layer. \n",
    "    \n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        \n",
    "        # check to make sure that no. of layers is one more than \n",
    "        # no. of activation functions because the input layer \n",
    "        # has no activation function.\n",
    "        \n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        \n",
    "        # define the local variables layers and activations\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        \n",
    "        # initialize weights and biases as two lists to hold\n",
    "        # weights and biases for each layer\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # create random weights for biases and weights\n",
    "        # for each of the layes.\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
    "    \n",
    "    # do feedforward for x\n",
    "    # where x is an input\n",
    "    \n",
    "    # return a list of a's and z's as expected.\n",
    "    # a's and z's are layer by layer\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # make a copy of x\n",
    "        a = np.copy(x)\n",
    "        \n",
    "        # this variable will contain all the z's\n",
    "        z_s = []\n",
    "        \n",
    "        # this variable will contain all the a's\n",
    "        # the output of the input layer is simply x which is the\n",
    "        # input. So we initialize the a_s to contain a. \n",
    "        a_s = [a]\n",
    "        \n",
    "        # for each layer do\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            # retrieve the appropriate activation function\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            \n",
    "            # create z_s by z = w.a + b for each layer\n",
    "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "            \n",
    "            # create z_a by a = f(row) -- \n",
    "            # note that we apply it to the last element only \n",
    "            # by using the -1 notation. We only want to apply\n",
    "            # the activation function to the last layer just \n",
    "            # added.\n",
    "            \n",
    "            a = activation_function(z_s[-1])\n",
    "            \n",
    "            # keep track of the new activation or a_s \n",
    "            a_s.append(a)\n",
    "            \n",
    "            # return both z_s and a_s \n",
    "            # we will have z_s and a_s for each layer \n",
    "        return (z_s, a_s)\n",
    "\n",
    "    \n",
    "    # takes the y -- the actual answer, a's and z's and \n",
    "    # calculates the dLoss/dw and dLoss/db\n",
    "    \n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "        \n",
    "        # initialize list of dLoss/dw and dLoss/db\n",
    "        dw = []  # dLoss/dw\n",
    "        db = []  # dLoss/db\n",
    "        \n",
    "        # create an empty list of deltas, one for each weight\n",
    "        deltas = [None] * len(self.weights)  # delta = dLoss/dz  known as error for each layer\n",
    "        \n",
    "        \n",
    "        # start from the back and insert the last layer error\n",
    "        # based on the square loss function. Note -1 is used to \n",
    "        # fill things from the back of the list \n",
    "        # also note that we need to use the derivative function \n",
    "        # for the activation function.\n",
    "        # note that we do not need to use the 2 in the loss function derivation\n",
    "        \n",
    "        # again note this is for the last layer only!\n",
    "        \n",
    "        deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "        \n",
    "        \n",
    "        # Perform BackPropagation\n",
    "        \n",
    "        # for the rest of the deltas, go in reverse order\n",
    "        for i in reversed(range(len(deltas)-1)):\n",
    "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        \n",
    "        \n",
    "        #a= [print(d.shape) for d in deltas]\n",
    "        \n",
    "        # now we need to update the weights based on the calculated\n",
    "        # deltas\n",
    "        \n",
    "        #now we will determine the batch size from the first dimension \n",
    "        #of shape of y. We simply want to see how many test cases are there\n",
    "        #for example there may be 10 y's; one for each x. \n",
    "        \n",
    "        batch_size = y.shape[1]\n",
    "        \n",
    "        # determine the two derivatives by taking \n",
    "        # the average according to batch sizes \n",
    "        \n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        \n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "        return dw, db\n",
    "\n",
    "    \n",
    "    # Now we will write the main training function that uses\n",
    "    # feedforward and backpropagation many times (called epochs)\n",
    "    # lr (learning rate) is the eta in our equations.\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "    \n",
    "    # update weights and biases based on the output\n",
    "    # for the number of epochs\n",
    "    \n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            \n",
    "            # Do the training in batches\n",
    "            # each batch is a subset of the original \n",
    "            # data \n",
    "            \n",
    "            while(i<len(y)):\n",
    "                \n",
    "                # extract a batch\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                # update i for the next batches\n",
    "                i = i+batch_size\n",
    "                \n",
    "                # do the feedforward for the batch and update the weights\n",
    "                # based on the average loss for each weight for the whole\n",
    "                # batch.\n",
    "                \n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                \n",
    "                # do the back propagation \n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                \n",
    "                \n",
    "                # update the weights for each pair of weights and dw\n",
    "                # and biases and db\n",
    "                \n",
    "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "                \n",
    "                # print the loss using a built in function \n",
    "                # to calculate the loss\n",
    "                print(\"loss = \", np.linalg.norm(a_s[-1]-y_batch) )\n",
    "    \n",
    "    \n",
    "    # This function is being used to return an activation function \n",
    "    # depending on its weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "    \n",
    "    # This function returns the derivative of a function depending\n",
    "    # on its name.\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights= [array([[0.05883504]]), array([[-0.20340329]])]\n",
      "z_s= [array([[0.89535767]]), array([[0.29381479]])]\n",
      "a_s= [array([4]), array([[0.89535767]]), array([[0.29381479]])]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let us try feedforward on one simple output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([1, 1, 1],activations=['relu', 'relu'])\n",
    "\n",
    "x = np.array([4])\n",
    "#x = [[1]] # 2 x 1 \n",
    "## Let us do one feedforward\n",
    "## Remember input is 1 row\n",
    "z, a = nn.feedforward(x)\n",
    "\n",
    "print(\"weights=\", nn.weights)  ## 3 x 2\n",
    "\n",
    "print(\"z_s=\",z)\n",
    "print(\"a_s=\",a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Consider the nerual network in Figure 2. Using a similar approach as Exercise 1, implement this neural network and print the predicted values of <b> $z_{i}$ and $a_{i}$ </b> after the first forward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex2.png\" style=\"width:400px;\"></center><caption><center><b>Figure 2</b>: Neural network with 1 input, 1 hidden layer with 3 nodes, and 1 output.<br></center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "- z= self.weights[i].dot(a) + self.biases[i]\n",
    "- a = activation_function(z_s[-1])\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 0.25$ for the given input $x_{1} = 0.5$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.15 & 0.45 & -0.35 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.23 & 0.65 & -0.15 \\end{bmatrix}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights= [array([[ 0.37686796],\n",
      "       [ 1.04729499],\n",
      "       [-1.19144963]]), array([[-0.97121173,  1.15703367,  0.78519048]])]\n",
      "z_s= [array([[-1.44664164, -1.11142813, -2.23080044],\n",
      "       [-0.19102765,  0.14418586, -0.97518645],\n",
      "       [-3.11100477, -2.77579125, -3.89516356]]), array([[0.96427358, 1.13110147, 0.96427358]])]\n",
      "a_s= [array([0.5]), array([[0.        , 0.        , 0.        ],\n",
      "       [0.        , 0.14418586, 0.        ],\n",
      "       [0.        , 0.        , 0.        ]]), array([[0.96427358, 1.13110147, 0.96427358]])]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Let us try feedforward on one simple output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([1, 3, 1],activations=['relu', 'relu'])\n",
    "\n",
    "x = np.array([0.5])\n",
    "#x = [[1]] # 2 x 1 \n",
    "## Let us do one feedforward\n",
    "## Remember input is 1 row\n",
    "z, a = nn.feedforward(x)\n",
    "\n",
    "print(\"weights=\", nn.weights)  ## 3 x 2\n",
    "\n",
    "print(\"z_s=\",z)\n",
    "print(\"a_s=\",a)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3**\n",
    "\n",
    "Consider the nerual network in Figure 3. Using a similar approach as the previous exercises,implement this neural network and print the predicted values of <b> $z_{i}$ and $a_{i}$ </b> after the first forward pass. \n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex3.png\" style=\"width:500px;\"></center><caption><center><b>Figure 3</b>: Neural network with 4 inputs, 2 hidden layers, and 1 output.<br></center></caption><br>\n",
    "\n",
    "\n",
    "**Note:**\n",
    "- z= self.weights[i].dot(a) + self.biases[i]\n",
    "- a = activation_function(z_s[-1])\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 7$ for the given inputs $x_{1} = 2$, $x_{2} = 3$, $x_{3} = 1$, $x_{4} = 1$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "- The initial <b>weights</b> are given as: \n",
    "\n",
    "w1 = $\\begin{bmatrix}\n",
    "0.15 & 0.45 \\\\\n",
    "-0.65 & -0.35 \\\\\n",
    "0.35 & 0.25 \\\\\n",
    "-0.55 & -0.75\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w2 = $\\begin{bmatrix}\n",
    "0.23 & 0.15 & 0.45 \\\\ \n",
    "-0.001 & -0.9 & 0.25 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w3 = $\\begin{bmatrix}0.1 \\\\  0.2 \\\\ -0.1 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights= [array([[ 1.26990974,  2.05154436,  0.95100371,  0.68794692],\n",
      "       [ 0.74603863, -1.40429693,  0.19958755,  0.45326056]]), array([[ 0.17739952, -0.02436658],\n",
      "       [ 1.54964989, -1.05501852],\n",
      "       [ 0.38061867, -1.90555293]]), array([[-0.22035439, -0.78576227,  1.49995501]])]\n",
      "z_s= [array([[ 9.2417968 , -3.15957182],\n",
      "       [10.68536311, -1.71600551]]), array([[  0.77709602,  -0.60202852],\n",
      "       [  3.68296401,   0.63467055],\n",
      "       [-17.54040742,  -0.69648283]]), array([[-3.47249543, -0.90602493]])]\n",
      "a_s= [array([2, 3, 1, 1]), array([[ 9.2417968 ,  0.        ],\n",
      "       [10.68536311,  0.        ]]), array([[0.77709602, 0.        ],\n",
      "       [3.68296401, 0.63467055],\n",
      "       [0.        , 0.        ]]), array([[0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "# Let us try feedforward on one simple output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([4, 2, 3, 1],activations=['relu', 'relu', 'relu'])\n",
    "\n",
    "x = np.array([2,3,1,1])\n",
    "#x = [[1]] # 2 x 1 \n",
    "## Let us do one feedforward\n",
    "## Remember input is 1 row\n",
    "z, a = nn.feedforward(x)\n",
    "\n",
    "print(\"weights=\", nn.weights)  ## 3 x 2\n",
    "\n",
    "print(\"z_s=\",z)\n",
    "print(\"a_s=\",a)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
