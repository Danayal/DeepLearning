{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/header.png\" ></center>\n",
    "\n",
    "<center><h3>Lab 5 - Exploring Neural Network Forward and Backward Propagations</h3></center>\n",
    "<br>\n",
    "<b> Objectives: </b>\n",
    "\n",
    "* To practice doing forward and backward passes\n",
    "\n",
    "* To practice using the backpropagation algorithm\n",
    "\n",
    "* To practice calculating the derivatives\n",
    "\n",
    "<b> Refer to your previous lab solution and to the lecture notes (particularly backprop_explore notebook) to solve the exercises </b>\n",
    "\n",
    "Check your solutions by solving it by hand following the steps (with the tables) in yesterday's lecture. <b> You need to submit this as part of your submission. </b>\n",
    "\n",
    "<b> Submit: </b>\n",
    "* Report with cover page and solution of the exercises by-hand\n",
    "* Jupyter notebook with the solutuion\n",
    "\n",
    "<b> Due Date: Sunday, 22nd March 2020, 11:59pm </b><br><br>\n",
    "<b><i>Please enter your Student ID & Name below:</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student ID: Danayal Khan\n",
    "## Student Name: b00069350"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and ReLU activation function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "# def relu_function(x):\n",
    "#     if (x<0):\n",
    "#         return 0\n",
    "#     else:\n",
    "#         return x\n",
    "\n",
    "def relu_df(x):\n",
    "    if (x>=0): \n",
    "         return 1\n",
    "    else: \n",
    "         return 0\n",
    "\n",
    "# def relu(x):\n",
    "#     t = map(relu_function, x)\n",
    "#     dt = map(relu_df,x)\n",
    "#     return t, dt\n",
    "\n",
    "\n",
    "# def relu(X):\n",
    "#    return np.maximum(0,X)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def reluDerivative(x):\n",
    "#     x[x<=0] = 0\n",
    "#     x[x>0] = 1\n",
    "#     return x\n",
    "\n",
    "# def loss(x,y):\n",
    "#     return (x-y)*(x-y)\n",
    "\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "   return np.maximum(0,X)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def loss(x,y):\n",
    "    return (x-y)*(x-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2 marks]**\n",
    "\n",
    "Consider the simple 3-layer neural network in Figure 1. By using a step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a^l$ </b>, after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex1.png\" style=\"width:400px;\"></center><caption><center><b>Figure 1</b>: 3-layer neural network.</center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 0.25$ for the given input $x_{1}$ = 0.5\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.15 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.23 \\end{bmatrix}$\n",
    "\n",
    "<i><b>Hints</b></i>:<br><br>\n",
    "\n",
    "For backward pass, you may want to recall that using Gradient descent (refer to lecture notes):\n",
    "\n",
    "- $w_{1\\, updated} = w_{1} - \\eta \\frac {\\partial Loss}{\\partial w_{1}}$\n",
    "- $w_{2\\, updated} = w_{2} - \\eta \\frac {\\partial Loss}{\\partial w_{2}}$\n",
    "\n",
    "\n",
    "Also refer to the steps in the last lecture (Exercises using BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " [0.5]\n",
      "y=\n",
      " 0.25\n",
      "z1=\n",
      " 0.075\n",
      "a1=\n",
      " 0.075\n",
      "z2=\n",
      " [0.01725]\n",
      "a2=\n",
      " [0.01725]\n",
      "del2=\n",
      " -0.4655\n",
      "del1=\n",
      " [-0.107065]\n",
      "dLoss/dW2=\n",
      " -0.0349125\n",
      "dLoss/dW1=\n",
      " -0.053532500000000004\n",
      "original w1=\n",
      " [0.15]\n",
      "original w2=\n",
      " [0.23]\n",
      "Updated w1=\n",
      " [0.15005353]\n",
      "Updated w2=\n",
      " [0.23003491]\n"
     ]
    }
   ],
   "source": [
    "# Exercise:  Calculate the forward and backward pass of the example \n",
    "# nn by hand using a step by step approach\n",
    "# imports and ReLU activation function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using arbirary weights\n",
    "w1 = np.array([0.15])\n",
    "w2 = np.array([0.23])\n",
    "\n",
    "# x and y\n",
    "x = np.array([0.5])\n",
    "y = 0.25\n",
    "\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\\n\",y)\n",
    "\n",
    "\n",
    "##  Forward Pass  ###\n",
    "\n",
    "# z(1) = x . W1\n",
    "z1 = np.dot(x,w1)\n",
    "print(\"z1=\\n\",z1)\n",
    "a1 = relu(z1)\n",
    "print(\"a1=\\n\",a1)\n",
    "\n",
    "# z2 = a1 . W2 T\n",
    "z2 = np.dot(a1,w2.T)\n",
    "print(\"z2=\\n\",z2)\n",
    "a2 = relu(z2)\n",
    "print(\"a2=\\n\",a2)\n",
    "\n",
    "## Backward Pass ###\n",
    "\n",
    "# calculate from last layer \n",
    "del2 = 2 * (a2[0]-y) * relu_df(z2[0])\n",
    "print(\"del2=\\n\", del2)\n",
    "\n",
    "# calculater del1 recursively\n",
    "\n",
    "del1 = w2 * del2 * relu_df(z1)\n",
    "print(\"del1=\\n\", del1)\n",
    "\n",
    "# calculate all the derivitives\n",
    "\n",
    "dlossdW2 = del2 * a1\n",
    "print(\"dLoss/dW2=\\n\", dlossdW2)\n",
    "dlossdW1 = np.dot(x.T,del1)\n",
    "print(\"dLoss/dW1=\\n\", dlossdW1)\n",
    "\n",
    "# update w1 and w2\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "print(\"original w1=\\n\", w1)\n",
    "print(\"original w2=\\n\", w2)\n",
    "\n",
    "# Update and print w1 and w2 \n",
    "\n",
    "w1 = w1 - eta*dlossdW1\n",
    "print(\"Updated w1=\\n\",w1)\n",
    "w2 = w2 - eta*dlossdW2\n",
    "print(\"Updated w2=\\n\",w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [4 marks]**\n",
    "\n",
    "Consider the nerual network in Figure 2. Using a similar step-by-step approach as Exercise 1, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a_{i}$ </b>, after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex2.png\" style=\"width:400px;\"></center><caption><center><b>Figure 2</b>: Neural network with 1 input, 1 hidden layer with 3 nodes, and 1 output.<br></center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 0.25$ for the given input $x_{1} = 0.5$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.15 & 0.45 & -0.35 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.23 \\\\ 0.65 \\\\-0.15 \\end{bmatrix}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " [[0.5]]\n",
      "y=\n",
      " 0.25\n",
      "z1=\n",
      " [[ 0.075  0.225 -0.175]]\n",
      "a1=\n",
      " [[0.075 0.225 0.   ]]\n",
      "z2=\n",
      " [0.1635]\n",
      "a2=\n",
      " [0.1635]\n",
      "del2=\n",
      " [-0.173]\n",
      "z1 =  [[ 0.075  0.225 -0.175]]\n",
      "del1=\n",
      " [[-0.03979 -0.11245  0.     ]]\n",
      "relue derivative of z1  [[1. 1. 0.]]\n",
      "dLoss/dW2=\n",
      " [[-0.012975 -0.038925 -0.      ]]\n",
      "dLoss/dW1=\n",
      " [[-0.019895 -0.056225  0.      ]]\n",
      "original w1=\n",
      " [[ 0.15  0.45 -0.35]]\n",
      "original w2=\n",
      " [ 0.23  0.65 -0.15]\n",
      "Updated w1=\n",
      " [[ 0.15001989  0.45005623 -0.35      ]]\n",
      "Updated w2=\n",
      " [[ 0.23001298  0.65003893 -0.15      ]]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "# Exercise:  Calculate the forward and backward pass of the example \n",
    "# nn by hand using a step by step approach\n",
    "# imports and ReLU activation function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using arbirary weights\n",
    "w1 = np.array([[0.15, 0.45, -0.35]])\n",
    "w2 = np.array([0.23, 0.65, -0.15])\n",
    "\n",
    "# x and y\n",
    "x = np.array([[0.5]])\n",
    "y = 0.25\n",
    "\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\\n\",y)\n",
    "\n",
    "\n",
    "##  Forward Pass  ###\n",
    "\n",
    "# z(1) = x . W1\n",
    "z1 = np.dot(x,w1)\n",
    "print(\"z1=\\n\",z1)\n",
    "a1 = relu(z1)\n",
    "print(\"a1=\\n\",a1)\n",
    "\n",
    "# z2 = a1 . W2 T\n",
    "z2 = np.dot(a1,w2)\n",
    "print(\"z2=\\n\",z2)\n",
    "a2 = relu(z2)\n",
    "print(\"a2=\\n\",a2)\n",
    "\n",
    "\n",
    "## Backward Pass ###\n",
    "\n",
    "# calculate from last layer \n",
    "\n",
    "\n",
    "del2 = 2 * (a2-y) * relu_df(z2)\n",
    "print(\"del2=\\n\", del2)\n",
    "\n",
    "# calculater del1 recursively\n",
    "\n",
    "print(\"z1 = \", z1)\n",
    "\n",
    "del1 = w2 * del2 * reluDerivative(z1)\n",
    "\n",
    "\n",
    "print(\"del1=\\n\", del1)\n",
    "\n",
    "print(\"relue derivative of z1 \", reluDerivative(z1))\n",
    "\n",
    "# calculate all the derivitives\n",
    "\n",
    "\n",
    "dlossdW2 = del2 * a1\n",
    "print(\"dLoss/dW2=\\n\", dlossdW2)\n",
    "dlossdW1 = np.dot(x.T,del1)\n",
    "print(\"dLoss/dW1=\\n\", dlossdW1)\n",
    "\n",
    "\n",
    "# update w1 and w2\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "print(\"original w1=\\n\", w1)\n",
    "print(\"original w2=\\n\", w2)\n",
    "\n",
    "# Update and print w1 and w2 \n",
    "\n",
    "w1 = w1 - eta*dlossdW1\n",
    "print(\"Updated w1=\\n\",w1)\n",
    "w2 = w2 - eta*dlossdW2\n",
    "print(\"Updated w2=\\n\",w2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 [4 marks]**\n",
    "\n",
    "Consider the nerual network in Figure 3. Using a similar step-by-step approach as the previous exercises, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a_{i}$ </b> after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex3.png\" style=\"width:500px;\"></center><caption><center><b>Figure 3</b>: Neural network with 4 inputs, 2 hidden layers, and 1 output.<br></center></caption><br>\n",
    "\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 7$ for the given inputs $x_{1} = 2$, $x_{2} = 3$, $x_{3} = 1$, $x_{4} = 1$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: \n",
    "\n",
    "w1 = $\\begin{bmatrix}\n",
    "0.15 & 0.45 \\\\\n",
    "-0.65 & -0.35 \\\\\n",
    "0.35 & 0.25 \\\\\n",
    "-0.55 & -0.75\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w2 = $\\begin{bmatrix}\n",
    "0.23 & 0.15 & 0.45 \\\\ \n",
    "-0.001 & -0.9 & 0.25 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w3 = $\\begin{bmatrix}0.1 \\\\  0.2 \\\\ -0.1 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2) (2, 3) (3,)\n",
      "(1, 4)\n",
      "x=\n",
      " [[2 3 1 1]]\n",
      "y=\n",
      " 7\n",
      "z1=\n",
      " [[-1.85 -0.65]]\n",
      "a1=\n",
      " [[0. 0.]]\n",
      "z2=\n",
      " [[0. 0. 0.]]\n",
      "a2=\n",
      " [[0. 0. 0.]]\n",
      "z3=\n",
      " [0.]\n",
      "a3=\n",
      " [0.]\n",
      "del3=\n",
      " [-0.]\n",
      "del2=\n",
      " [[-0. -0.  0.]]\n",
      "del1=\n",
      " [[0. 0.]]\n",
      "dLoss/dW3=\n",
      " [[-0. -0. -0.]]\n",
      "dLoss/dW2=\n",
      " [[-0. -0.]\n",
      " [-0. -0.]\n",
      " [ 0.  0.]]\n",
      "(4, 1)\n",
      "(1, 2)\n",
      "dLoss/dW1=\n",
      " [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n",
      "original w1=\n",
      " [[ 0.15  0.45]\n",
      " [-0.65 -0.35]\n",
      " [ 0.35  0.25]\n",
      " [-0.55 -0.75]]\n",
      "original w2=\n",
      " [[ 0.23   0.15   0.45 ]\n",
      " [-0.001 -0.9    0.25 ]]\n",
      "original w3=\n",
      " [ 0.1  0.2 -0.1]\n",
      "Updated w1=\n",
      " [[ 0.15  0.45]\n",
      " [-0.65 -0.35]\n",
      " [ 0.35  0.25]\n",
      " [-0.55 -0.75]]\n",
      "Updated w2=\n",
      " [[ 0.23   0.15   0.45 ]\n",
      " [-0.001 -0.9    0.25 ]]\n",
      "Updated w3=\n",
      " [[ 0.1  0.2 -0.1]]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "# Exercise:  Calculate the forward and backward pass of the example \n",
    "# nn by hand using a step by step approach\n",
    "# imports and ReLU activation function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# using arbirary weights\n",
    "\n",
    "#w1 shape will be (4,2)\n",
    "#w2 shape will be (2,3)\n",
    "#w3 shape will be (3,1)\n",
    "\n",
    "\n",
    "w1 = np.array( [ [ [0.15] , [0.45] ],[ [-0.65] , [-0.35]] ,[ [0.35] , [0.25] ], [[-0.55] , [-0.75]] ] )\n",
    "\n",
    "w2 = np.array([ [  [ 0.23],[0.15 ],[0.45 ]           ], [ [-0.001],[-0.9],[0.25]    ]   ])\n",
    "\n",
    "w3 = np.array (  [0.1, 0.2, -0.1]             )\n",
    "\n",
    "\n",
    "w1 = w1[:,:,0]\n",
    "w2 = w2[:,:,0]\n",
    "\n",
    "print(w1.shape, w2.shape, w3.shape)\n",
    "# x and y\n",
    "x = np.array([[2,3,1,1]])\n",
    "print(x.shape)\n",
    "y = 7\n",
    "\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\\n\",y)\n",
    "\n",
    "\n",
    "##  Forward Pass  ###\n",
    "\n",
    "# z(1) = x . W1\n",
    "\n",
    "z1 = np.dot(x,w1)\n",
    "print(\"z1=\\n\",z1)\n",
    "a1 = relu(z1)\n",
    "print(\"a1=\\n\",a1)\n",
    "\n",
    "# z2 = a1 . W2 T\n",
    "z2 = np.dot(a1,w2)\n",
    "print(\"z2=\\n\",z2)\n",
    "a2 = relu(z2)\n",
    "print(\"a2=\\n\",a2)\n",
    "\n",
    "#z3 = a2. W3 T\n",
    "z3 = np.dot(a2,w3)\n",
    "print(\"z3=\\n\",z3)\n",
    "a3 = relu(z3)\n",
    "print(\"a3=\\n\",a3)\n",
    "\n",
    "\n",
    "## Backward Pass ###\n",
    "\n",
    "\n",
    "# calculate from last layer \n",
    "del3 = 2 * (a3-y) * reluDerivative(z3)\n",
    "print(\"del3=\\n\", del3)\n",
    "\n",
    "\n",
    "# calculate del2 and del1 recursively\n",
    "\n",
    "\n",
    "\n",
    "del2 = w3.T * del3 * reluDerivative(z2)\n",
    "\n",
    "print(\"del2=\\n\", del2)\n",
    "\n",
    "\n",
    "\n",
    "del1 = (del2 @ w2.T) * reluDerivative(z1) \n",
    "\n",
    "\n",
    "print(\"del1=\\n\", del1)\n",
    "\n",
    "\n",
    "# calculate all the derivitives\n",
    "\n",
    "\n",
    "\n",
    "dlossdW3 = del3.T * a2\n",
    "print(\"dLoss/dW3=\\n\", dlossdW3)\n",
    "\n",
    "dlossdW2 = del2.T * a1\n",
    "print(\"dLoss/dW2=\\n\", dlossdW2)\n",
    "\n",
    "\n",
    "print(x.T.shape)\n",
    "print(del1.shape)\n",
    "\n",
    "dlossdW1 = x.T @ del1\n",
    "print(\"dLoss/dW1=\\n\", dlossdW1)\n",
    "\n",
    "\n",
    "\n",
    "# update w1 and w2\n",
    "\n",
    "eta = 0.001\n",
    "\n",
    "print(\"original w1=\\n\", w1)\n",
    "print(\"original w2=\\n\", w2)\n",
    "print(\"original w3=\\n\", w3)\n",
    "\n",
    "\n",
    "# Update and print w1 and w2 \n",
    "\n",
    "w1 = w1 - eta*dlossdW1\n",
    "print(\"Updated w1=\\n\",w1)\n",
    "w2 = w2 - eta*dlossdW2.T\n",
    "print(\"Updated w2=\\n\",w2)\n",
    "w3 = w3 - eta*dlossdW3\n",
    "print(\"Updated w3=\\n\",w3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
