{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"imgs/header.png\" ></center>\n",
    "\n",
    "<center><h3>Lab 5 - Exploring Neural Network Forward and Backward Propagations</h3></center>\n",
    "<br>\n",
    "<b> Objectives: </b>\n",
    "\n",
    "* To practice doing forward and backward passes\n",
    "\n",
    "* To practice using the backpropagation algorithm\n",
    "\n",
    "* To practice calculating the derivatives\n",
    "\n",
    "<b> Refer to your previous lab solution and to the lecture notes (particularly backprop_explore notebook) to solve the exercises </b>\n",
    "\n",
    "Check your solutions by solving it by hand following the steps (with the tables) in yesterday's lecture. <b> You need to submit this as part of your submission. </b>\n",
    "\n",
    "<b> Submit: </b>\n",
    "* Report with cover page and solution of the exercises by-hand\n",
    "* Jupyter notebook with the solutuion\n",
    "\n",
    "<b> Due Date: Sunday, 22nd March 2020, 11:59pm </b><br><br>\n",
    "<b><i>Please enter your Student ID & Name below:</i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student ID: \n",
    "## Student Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and ReLU activation function\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "def relu_function(x):\n",
    "    if (x<0):\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "def relu_df(x):\n",
    "    if (x>=0): \n",
    "         return 1\n",
    "    else: \n",
    "         return 0\n",
    "\n",
    "def relu(x):\n",
    "    t = map(relu_function, x)\n",
    "    dt = map(relu_df,x)\n",
    "    return t, dt\n",
    "\n",
    "\n",
    "def relu(X):\n",
    "   return np.maximum(0,X)\n",
    "\n",
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def loss(x,y):\n",
    "    return (x-y)*(x-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2 marks]**\n",
    "\n",
    "Consider the simple 3-layer neural network in Figure 1. By using a step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a^l$ </b>, after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex1.png\" style=\"width:400px;\"></center><caption><center><b>Figure 1</b>: 3-layer neural network.</center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 0.25$ for the given input $x_{1}$ = 0.5\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.15 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.23 \\end{bmatrix}$\n",
    "\n",
    "<i><b>Hints</b></i>:<br><br>\n",
    "\n",
    "For backward pass, you may want to recall that using Gradient descent (refer to lecture notes):\n",
    "\n",
    "- $w_{1\\, updated} = w_{1} - \\eta \\frac {\\partial Loss}{\\partial w_{1}}$\n",
    "- $w_{2\\, updated} = w_{2} - \\eta \\frac {\\partial Loss}{\\partial w_{2}}$\n",
    "\n",
    "\n",
    "Also refer to the steps in the last lecture (Exercises using BP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [4 marks]**\n",
    "\n",
    "Consider the nerual network in Figure 2. Using a similar step-by-step approach as Exercise 1, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a_{i}$ </b>, after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex2.png\" style=\"width:400px;\"></center><caption><center><b>Figure 2</b>: Neural network with 1 input, 1 hidden layer with 3 nodes, and 1 output.<br></center></caption><br>\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 0.25$ for the given input $x_{1} = 0.5$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: <br>\n",
    "w1 = $\\begin{bmatrix}0.15 & 0.45 & -0.35 \\end{bmatrix}$\n",
    "<br>\n",
    "w2 = $\\begin{bmatrix}0.23 & 0.65 & -0.15 \\end{bmatrix}$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 [4 marks]**\n",
    "\n",
    "Consider the nerual network in Figure 3. Using a similar step-by-step approach as the previous exercises, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print: <br>\n",
    "<b>1)</b> the predicted values of <b> $z_{i}$ and $a_{i}$ </b> after the first forward pass (from lab 4), and <br>\n",
    "<b>2)</b> the updated values of the <b>weights</b> after the first backward pass.\n",
    "\n",
    "<center><img src=\"imgs/Lab5_Ex3.png\" style=\"width:500px;\"></center><caption><center><b>Figure 3</b>: Neural network with 4 inputs, 2 hidden layers, and 1 output.<br></center></caption><br>\n",
    "\n",
    "\n",
    "**Note:**\n",
    "- $z^{l}$ = $a_{i}$.$w_{i}$ and $a^{l}$ = σ ($z^{l})$\n",
    "\n",
    "- For this exercise, use the <b> ReLU activation</b> function: $f(z)=max(0,z)$\n",
    "\n",
    "- Assume the actual output value $y = 7$ for the given inputs $x_{1} = 2$, $x_{2} = 3$, $x_{3} = 1$, $x_{4} = 1$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = (x-y)^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial x} = 2(x-y)$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.001$\n",
    "\n",
    "- The initial <b>weights</b> are given as: \n",
    "\n",
    "w1 = $\\begin{bmatrix}\n",
    "0.15 & 0.45 \\\\\n",
    "-0.65 & -0.35 \\\\\n",
    "0.35 & 0.25 \\\\\n",
    "-0.55 & -0.75\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w2 = $\\begin{bmatrix}\n",
    "0.23 & 0.15 & 0.45 \\\\ \n",
    "-0.001 & -0.9 & 0.25 \\\\\n",
    "\\end{bmatrix}$\n",
    "\n",
    "\n",
    "w3 = $\\begin{bmatrix}0.1 \\\\  0.2 \\\\ -0.1 \\end{bmatrix}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
