{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/CourseTitle.png\" style=\"width:900px;\"></center>\n",
    "\n",
    "<center><h2>Homework 2 - Exploring Neural Network Forward and Backward Propagations</h2></center>\n",
    "<br>\n",
    "<center>COE49412 - Spring 2020</center>\n",
    "<br><b>\n",
    "Total Marks: 10<br>\n",
    "Due Date: Monday, 23-Mar-2020, 11.59pm</b><br><br>\n",
    "<b><i><u>Please enter your Student ID & Name below:</u><i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student ID: \n",
    "## Student Name: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2 marks]**\n",
    "\n",
    "Consider the following simple 2-layer neural network architecture containing one input, one hidden layer with a single node, and one output. By using a step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted value, $a_{2}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX1.png\" style=\"width:600px;\"><caption><center><b>Figure 1</b>: 2-layer neural network. <i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth value $y = 0.25$ for the given input $x = 0.1$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{2})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{2}} = -(y - a_{2})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as: $\\begin{bmatrix} w_{1}=0.15 & w_{2}=0.45 \\end{bmatrix}$\n",
    "\n",
    "- The initial <b>biases</b>, $b$, are given as: $\\begin{bmatrix} b_{1}=0.40 & b_{2}=0.65 \\end{bmatrix}$ \n",
    "\n",
    "<i><b>Hints</b></i>:<br><br>\n",
    "For backward pass, you may want to recall that using Gradient descent (refer your lecture notes):\n",
    "\n",
    "- $w_{1\\, updated} = w_{1} - \\eta \\frac {\\partial Loss}{\\partial w_{1}}$\n",
    "- $w_{2\\, updated} = w_{2} - \\eta \\frac {\\partial Loss}{\\partial w_{2}}$\n",
    "- $b_{1\\, updated} = b_{1} - \\eta \\frac {\\partial Loss}{\\partial b_{1}}$\n",
    "- $b_{2\\, updated} = b_{2} - \\eta \\frac {\\partial Loss}{\\partial b_{2}}$\n",
    "- And using the chain rule:\n",
    "    - $\\frac {\\partial Loss}{\\partial w_{2}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial w_{2}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot a_{1}$\n",
    "    - $\\frac {\\partial Loss}{\\partial b_{2}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial b_{2}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot 1$\n",
    "    - $\\frac {\\partial Loss}{\\partial w_{1}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial a_{1}} \\cdot \\frac {\\partial a_{1}}{\\partial z_{1}}  \\cdot \\frac {\\partial z_{1}}{\\partial w_{1}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot (w_{2}) \\cdot (a_{1} (1-a_{1})) \\cdot x_{1}$\n",
    "    - $\\frac {\\partial Loss}{\\partial b_{1}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial a_{1}} \\cdot \\frac {\\partial a_{1}}{\\partial z_{1}}  \\cdot \\frac {\\partial z_{1}}{\\partial b_{1}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot (w_{2}) \\cdot (a_{1} (1-a_{1})) \\cdot 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:20%\">\n",
    "    <tr>\n",
    "        <td><center><b>$a_{2}$</b></center></td> \n",
    "        <td><center></b>0.7153</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$w_{1\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.1496</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$w_{2\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.4272</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$b_{1\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.3959</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$b_{2\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.6121</b></center></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [3 marks]**\n",
    "\n",
    "Consider now a slightly larger variation of the above neural network architecture containing two inputs, one hidden layer with three nodes, and one output. By using a similar step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted value, $a_{4}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX2.png\" style=\"width:600px;\"><caption><center><b>Figure 2</b>: Neural network with 2 inputs, 1 hidden layer with 3 nodes, and 1 output.<br><i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth value $y = 0.25$ for the given inputs $x_{1} = 0.1$ and $x_{2} = 0.35$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{4})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{4}} = -(y - a_{4})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as: $\\begin{bmatrix} w_{1}=0.15 & w_{2}=0.45 & w_{3}=-0.35 & w_{4}=-0.61 & w_{5}=-0.52 & w_{6}=0.22 & w_{7}=-0.72 & w_{8}=1.22 & w_{9}=-0.30 \\end{bmatrix}$\n",
    "\n",
    "- The initial <b>biases</b>, $b$, are given as: $\\begin{bmatrix} b_{1}=0.40 & b_{2}=0.65 & b_{3}=0.23 & b_{4}=0.15 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:50%\">\n",
    "    <tr>\n",
    "        <td><center><b>$a_{4}$</b></center></td> \n",
    "        <td><center></b>0.5625</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>$W_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.1505\\\\ 0.4518\\\\ \\text{-}0.3509\\\\ \\text{-}0.6131\\\\ \\text{-}0.5198\\\\0.2208\\end{bmatrix}$</center></td> \n",
    "        <td><center>$W_{Output Layer\\, updated} = \\begin{bmatrix}\\text{-}0.7397\\\\ 1.202\\\\ \\text{-}0.3173\\end{bmatrix}$</center></td>     \n",
    "</tr>\n",
    "    <tr>\n",
    "        <td><center>$b_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.4051\\\\ 0.6410\\\\ 0.2323\\end{bmatrix}$</center></td> \n",
    "        <td><center>$b_{Output Layer\\, updated} = \\begin{bmatrix}0.1192\\end{bmatrix}$</center></td>     \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 [3 marks]**\n",
    "\n",
    "Consider now a more general version of the above neural network architecture containing two inputs, two hidden layers, and two outputs. By using a <i>generalized</i> approach, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted values, $a_{8}$ and $a_{9}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX3.png\" style=\"width:1000px;\"><caption><center><b>Figure 3</b>: Neural network with 2 inputs, 2 hidden layers, and 2 outputs.<br><i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth values $y_{1} = 0.25$ and $y_{2} = 0.4$ for the given inputs $x_{1} = 0.1$ and $x_{2} = 0.35$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{output})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{output}} = -(y - a_{output})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as:\n",
    "    - $W_{Layer\\, 1:\\, w_{1}\\,\\text{to}\\,w_{6}} = \\begin{bmatrix}0.45 & 0.83 & 0.62 & 0.84 & 0.22 & 0.86\\end{bmatrix}^T$\n",
    "    - $W_{Layer\\, 2:\\, w_{7}\\,\\text{to}\\,w_{18}} = \\begin{bmatrix}0.78 & 1.03 & 0.43 & 0.98 & 0.39 & 0.42 & -0.05 & 1.11 & 0.99 & 0.79 & 0.52 & 1.08\\end{bmatrix}^T$\n",
    "    - $W_{Output\\, Layer:\\, w_{19}\\,\\text{to}\\,w_{26}} = \\begin{bmatrix}0.79 & 0.62 & -0.32 & 0.65 & 0.52 & -0.78 & 0.41 & 0.91\\end{bmatrix}^T$\n",
    "- The initial <b>biases</b>, $b$, are given as:\n",
    "    - $b_{Layer\\, 1:\\, b_{1}\\,\\text{to}\\,b_{3}} = \\begin{bmatrix}0.1 & 1.03 & 0.59\\end{bmatrix}^T$\n",
    "    - $b_{Layer\\, 2:\\, b_{4}\\,\\text{to}\\,b_{7}} = \\begin{bmatrix}0.47 & 0.29 & -0.51 & 1.15\\end{bmatrix}^T$\n",
    "    - $b_{Output\\, Layer:\\, b_{8}\\,\\text{to}\\,b_{9}} = \\begin{bmatrix}0.92 & -0.6\\end{bmatrix}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:70%\">\n",
    "    <tr>\n",
    "        <td colspan=3><center><b>$a_{8}$</b></center></td> \n",
    "        <td colspan=3><center></b>0.9244</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=3><center><b>$a_{9}$</b></center></td> \n",
    "        <td colspan=3><center></b>0.5956</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td colspan=2><center>$W_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.4499\\\\0.8297\\\\0.6199\\\\0.8398\\\\0.2199\\\\0.8597\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$W_{Layer\\, 2\\, updated} = \\begin{bmatrix}0.7785\\\\1.0280\\\\0.4282\\\\0.9802\\\\0.3903\\\\0.4203\\\\-0.0501\\\\1.1097\\\\0.9897\\\\0.7890\\\\0.5187\\\\1.0788\\end{bmatrix}$</center></td> \n",
    "    <td colspan=2><center>$W_{Output Layer\\, updated} = \\begin{bmatrix}0.7732\\\\0.6046\\\\-0.3339\\\\0.6322\\\\0.5032\\\\-0.7954\\\\0.3960\\\\0.8922\\end{bmatrix}$</center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td colspan=2><center>$b_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.0993\\\\1.0294\\\\0.5894\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$b_{Layer\\, 2\\, updated} = \\begin{bmatrix}0.4675\\\\0.2904\\\\-0.5103\\\\1.1484\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$b_{Output Layer\\, updated} = \\begin{bmatrix}0.9011\\\\-0.6188\\end{bmatrix}$</center></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4 [2 marks]**\n",
    "\n",
    "Consider now replacing the <b>sigmoid activation function</b> in Exercise 3 with the <b>softmax activation function</b>: $f(z)=\\frac{e^z}{\\sum_i e^z}$ in the <b>output layer</b>, and the <b>ReLU activation function</b>: $f(z)=max(0,z)$ in the two <b>hidden layers</b>. Run your code from Exercise 3 for <b>50 iterations</b> and plot the $Loss$ function against the number of iterations. Compare your results with the <b>sigmoid</b> activation based hidden layers architecture in Exercise 3 and comment on your observations. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<i><b>Hints</b></i>:<br>\n",
    "- Account for the vanishing gradient problem\n",
    "- Recall that the derivative of the <b>softmax</b> function $f(z)=\\frac{e^z}{\\sum_i e^z}$ is $f'(z)=\\frac{e^z}{\\sum_i e^z} - \\frac{(e^z)^2}{(\\sum_i e^z)^2}$\n",
    "- Recall that the derivative of the <b>ReLU</b> function $f(z)=max(0,z)$ is $f(z) = \\left\\{\\begin{matrix}0 \\;\\;\\text{if} \\; z<0\\\\1 \\;\\;\\text{if} \\; z>0\\end{matrix}\\right.$\n",
    "- Sample indicative output plot shown below (your values may differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "\n",
    "<img src=\"images/LossFunction.png\" style=\"width:400px;\"><caption><center><b>Figure 4</b>: Sample indicative plot of $Loss$ function vs. number of iterations (your values may differ)</center></caption><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
