{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/CourseTitle.png\" style=\"width:900px;\"></center>\n",
    "\n",
    "<center><h2>Homework 2 - Exploring Neural Network Forward and Backward Propagations</h2></center>\n",
    "<br>\n",
    "<center>COE49412 - Spring 2020</center>\n",
    "<br><b>\n",
    "Total Marks: 10<br>\n",
    "Due Date: Monday, 23-Mar-2020, 11.59pm</b><br><br>\n",
    "<b><i><u>Please enter your Student ID & Name below:</u><i></b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Student ID: b00069350\n",
    "## Student Name: Danayal Khan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports and Sigmoid, Sigmoid derivative, softmax activation functions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(X):\n",
    "   return 1/(1+np.exp(-X))\n",
    "\n",
    "\n",
    "def sigmoid_df(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))\n",
    "\n",
    "\n",
    "def softmax(X):\n",
    "    expo = np.exp(X)\n",
    "    expo_sum = np.sum(np.exp(X))\n",
    "    return expo/expo_sum\n",
    "\n",
    "def loss(x,y):\n",
    "    return 0.5*(x-y)*(x-y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 [2 marks]**\n",
    "\n",
    "Consider the following simple 2-layer neural network architecture containing one input, one hidden layer with a single node, and one output. By using a step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted value, $a_{2}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX1.png\" style=\"width:600px;\"><caption><center><b>Figure 1</b>: 2-layer neural network. <i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth value $y = 0.25$ for the given input $x = 0.1$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{2})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{2}} = -(y - a_{2})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as: $\\begin{bmatrix} w_{1}=0.15 & w_{2}=0.45 \\end{bmatrix}$\n",
    "\n",
    "- The initial <b>biases</b>, $b$, are given as: $\\begin{bmatrix} b_{1}=0.40 & b_{2}=0.65 \\end{bmatrix}$ \n",
    "\n",
    "<i><b>Hints</b></i>:<br><br>\n",
    "For backward pass, you may want to recall that using Gradient descent (refer your lecture notes):\n",
    "\n",
    "- $w_{1\\, updated} = w_{1} - \\eta \\frac {\\partial Loss}{\\partial w_{1}}$\n",
    "- $w_{2\\, updated} = w_{2} - \\eta \\frac {\\partial Loss}{\\partial w_{2}}$\n",
    "- $b_{1\\, updated} = b_{1} - \\eta \\frac {\\partial Loss}{\\partial b_{1}}$\n",
    "- $b_{2\\, updated} = b_{2} - \\eta \\frac {\\partial Loss}{\\partial b_{2}}$\n",
    "- And using the chain rule:\n",
    "    - $\\frac {\\partial Loss}{\\partial w_{2}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial w_{2}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot a_{1}$\n",
    "    - $\\frac {\\partial Loss}{\\partial b_{2}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial b_{2}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot 1$\n",
    "    - $\\frac {\\partial Loss}{\\partial w_{1}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial a_{1}} \\cdot \\frac {\\partial a_{1}}{\\partial z_{1}}  \\cdot \\frac {\\partial z_{1}}{\\partial w_{1}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot (w_{2}) \\cdot (a_{1} (1-a_{1})) \\cdot x_{1}$\n",
    "    - $\\frac {\\partial Loss}{\\partial b_{1}} = \\frac {\\partial Loss}{\\partial a_{2}} \\cdot \\frac {\\partial a_{2}}{\\partial z_{2}} \\cdot \\frac {\\partial z_{2}}{\\partial a_{1}} \\cdot \\frac {\\partial a_{1}}{\\partial z_{1}}  \\cdot \\frac {\\partial z_{1}}{\\partial b_{1}} = (-(y - a_{2})) \\cdot (a_{2} (1-a_{2})) \\cdot (w_{2}) \\cdot (a_{1} (1-a_{1})) \\cdot 1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=\n",
      " [[0.1]]\n",
      "y=\n",
      " 0.25\n",
      "z1=\n",
      " [0.415]\n",
      "a1=\n",
      " [0.60228618]\n",
      "z2=\n",
      " [0.92102878]\n",
      "a2=\n",
      " [0.71525168]\n",
      "del2=\n",
      " [0.09475628]\n",
      "del1=\n",
      " [0.01021396]\n",
      "dLoss/dW2=\n",
      " [0.0570704]\n",
      "dLoss/dW1=\n",
      " [0.0010214]\n",
      "original w1=\n",
      " [0.15]\n",
      "original w2=\n",
      " [0.45]\n",
      "original b1=\n",
      " [0.4]\n",
      "original b2=\n",
      " [0.65]\n",
      "Updated w1=\n",
      " [0.14959144]\n",
      "Updated w2=\n",
      " [0.42717184]\n",
      "Updated b1 =\n",
      " [0.39591442]\n",
      "Updated b2 =\n",
      " [0.61209749]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "#weights\n",
    "w1 = np.array([0.15])\n",
    "w2 = np.array([0.45])\n",
    "\n",
    "#bias \n",
    "b1 = np.array([0.40])\n",
    "b2 = np.array([0.65])\n",
    "\n",
    "# x and y\n",
    "x = np.array([[0.1]])\n",
    "y = 0.25\n",
    "\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\\n\",y)\n",
    "\n",
    "\n",
    "##  Forward Pass  ###\n",
    "\n",
    "# z(1) = x . W1\n",
    "z1 = np.dot(x,w1) + b1\n",
    "print(\"z1=\\n\",z1) \n",
    "a1 = sigmoid(z1) \n",
    "print(\"a1=\\n\",a1)\n",
    "\n",
    "# z2 = a1 . W2 T\n",
    "z2 = np.dot(a1,w2.T) + b2\n",
    "print(\"z2=\\n\",z2)\n",
    "a2 = sigmoid(z2)\n",
    "print(\"a2=\\n\",a2)\n",
    "\n",
    "## Backward Pass ###\n",
    "\n",
    "\n",
    "# calculate delweight from last layer \n",
    "del2 = 1 * (a2-y) * sigmoid_df(z2)\n",
    "print(\"del2=\\n\", del2)\n",
    "\n",
    "# calculater del1 weight recursively\n",
    "del1 = w2 * del2 * sigmoid_df(z1)\n",
    "print(\"del1=\\n\", del1)\n",
    "\n",
    "\n",
    "# calculate all the derivitives for weights\n",
    "dlossdW2 = del2 * a1\n",
    "print(\"dLoss/dW2=\\n\", dlossdW2)\n",
    "dlossdW1 = np.dot(x.T,del1)\n",
    "print(\"dLoss/dW1=\\n\", dlossdW1)\n",
    "\n",
    "\n",
    "dlossB2 = del2\n",
    "dlossB1 = del1\n",
    "\n",
    "# update w1 and w2\n",
    "\n",
    "eta = 0.4\n",
    "\n",
    "print(\"original w1=\\n\", w1)\n",
    "print(\"original w2=\\n\", w2)\n",
    "\n",
    "\n",
    "print(\"original b1=\\n\", b1)\n",
    "print(\"original b2=\\n\", b2)\n",
    "\n",
    "# Update and print w1 and w2 \n",
    "\n",
    "w1 = w1 - eta*dlossdW1\n",
    "print(\"Updated w1=\\n\",w1)\n",
    "w2 = w2 - eta*dlossdW2\n",
    "print(\"Updated w2=\\n\",w2)\n",
    "\n",
    "\n",
    "# Update and print b1 and b2\n",
    "b1 = b1 - eta*dlossB1\n",
    "print (\"Updated b1 =\\n\", b1)\n",
    "\n",
    "b2 = b2 - eta*dlossB2\n",
    "print (\"Updated b2 =\\n\", b2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:20%\">\n",
    "    <tr>\n",
    "        <td><center><b>$a_{2}$</b></center></td> \n",
    "        <td><center></b>0.7153</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$w_{1\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.1496</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$w_{2\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.4272</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$b_{1\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.3959</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center><b>$b_{2\\, updated}$</b></center></td> \n",
    "        <td><center></b>0.6121</b></center></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 [3 marks]**\n",
    "\n",
    "Consider now a slightly larger variation of the above neural network architecture containing two inputs, one hidden layer with three nodes, and one output. By using a similar step-by-step approach by hand, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted value, $a_{4}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX2.png\" style=\"width:600px;\"><caption><center><b>Figure 2</b>: Neural network with 2 inputs, 1 hidden layer with 3 nodes, and 1 output.<br><i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth value $y = 0.25$ for the given inputs $x_{1} = 0.1$ and $x_{2} = 0.35$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{4})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{4}} = -(y - a_{4})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as: $\\begin{bmatrix} w_{1}=0.15 & w_{2}=0.45 & w_{3}=-0.35 & w_{4}=-0.61 & w_{5}=-0.52 & w_{6}=0.22 & w_{7}=-0.72 & w_{8}=1.22 & w_{9}=-0.30 \\end{bmatrix}$\n",
    "\n",
    "- The initial <b>biases</b>, $b$, are given as: $\\begin{bmatrix} b_{1}=0.40 & b_{2}=0.65 & b_{3}=0.23 & b_{4}=0.15 \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w1 shape (2, 3)\n",
      "w2 shape (3, 1)\n",
      "b1 shape (1, 3)\n",
      "b1 shape (1, 1)\n",
      "x=\n",
      " [[0.1  0.35]]\n",
      "y=\n",
      " 0.25\n",
      "z1=\n",
      " [[0.5725 0.4015 0.255 ]]\n",
      "a1=\n",
      " [[0.63933984 0.599048   0.56340679]]\n",
      "z2=\n",
      " [[0.25149184]]\n",
      "a2=\n",
      " [[0.56254366]]\n",
      "\n",
      "z2 shape (1, 1)\n",
      "\n",
      "a2-y shape (1, 1)\n",
      "\n",
      "del2 shape (1, 1)\n",
      "\n",
      "z1 shape  (1, 3)\n",
      "\n",
      " w2 shape (3, 1)\n",
      "del2=\n",
      " [[0.07691333]]\n",
      "del1=\n",
      " [[-0.01276921  0.02253801 -0.00567573]]\n",
      "(1, 3)\n",
      "(2, 1)\n",
      "dLoss/dW2=\n",
      " [[0.04917376 0.04607478 0.04333349]]\n",
      "dLoss/dW1=\n",
      " [[-0.00127692  0.0022538  -0.00056757]\n",
      " [-0.00446922  0.0078883  -0.00198651]]\n",
      "original w1=\n",
      " [[ 0.15 -0.35 -0.52]\n",
      " [ 0.45 -0.61  0.22]]\n",
      "original w2=\n",
      " [[-0.72]\n",
      " [ 1.22]\n",
      " [-0.3 ]]\n",
      "original b1=\n",
      " [[0.4  0.65 0.23]]\n",
      "original b2=\n",
      " [[0.15]]\n",
      "\n",
      "Updated w1=\n",
      " [[ 0.15051077 -0.35090152 -0.51977297]\n",
      " [ 0.45178769 -0.61315532  0.2207946 ]]\n",
      "Updated w2=\n",
      " [[-0.7396695   1.20157009 -0.3173334 ]]\n",
      "Updated b1 =\n",
      " [[0.40510768 0.6409848  0.23227029]]\n",
      "Updated b2 =\n",
      " [[0.11923467]]\n"
     ]
    }
   ],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "#weights\n",
    "\n",
    "#weight one will be of shape 2 x 3\n",
    "#weight 2 will be of shape 3 x 1\n",
    "w1 = np.array( [  [0.15,-0.35, -0.52 ] , [0.45,-0.61 ,0.22 ]] )\n",
    "\n",
    "\n",
    "w2 = np.array([[-0.72], [1.22],[-0.30]])\n",
    "\n",
    "b1 = np.array ([[0.40, 0.65, 0.23]])\n",
    "b2 = np.array ([[0.15]])\n",
    "\n",
    "print(\"w1 shape\", w1.shape)\n",
    "print(\"w2 shape\", w2.shape)\n",
    "\n",
    "print(\"b1 shape\", b1.shape)\n",
    "print(\"b1 shape\", b2.shape)\n",
    "\n",
    "# x and y\n",
    "x = np.array([[0.1, 0.35]])\n",
    "y = 0.25\n",
    "\n",
    "print(\"x=\\n\",x)\n",
    "print(\"y=\\n\",y)\n",
    "\n",
    "\n",
    "##  Forward Pass  ###\n",
    "\n",
    "# z(1) = x . W1\n",
    "z1 = np.dot(x,w1) + b1\n",
    "print(\"z1=\\n\",z1) \n",
    "a1 = sigmoid(z1) \n",
    "print(\"a1=\\n\",a1)\n",
    "\n",
    "# z2 = a1 . W2 T\n",
    "z2 = np.dot(a1,w2) + b2\n",
    "print(\"z2=\\n\",z2)\n",
    "\n",
    "a2 = sigmoid(z2)\n",
    "print(\"a2=\\n\",a2)\n",
    "\n",
    "## Backward Pass ###\n",
    "\n",
    "\n",
    "print(\"\\nz2 shape\", z2.shape)\n",
    "print(\"\\na2-y shape\", (a2-y).shape)\n",
    "print(\"\\ndel2 shape\", del2.shape)\n",
    "\n",
    "print(\"\\nz1 shape \", z1.shape)\n",
    "print(\"\\n w2 shape\", w2.shape)\n",
    "\n",
    "\n",
    "\n",
    "# calculate delweight from last layer \n",
    "del2 = 1 * (a2-y) * sigmoid_df(z2)\n",
    "print(\"del2=\\n\", del2)\n",
    "\n",
    "# calculater del1 weight recursively\n",
    "del1 = w2.T * del2 * sigmoid_df(z1)\n",
    "print(\"del1=\\n\", del1)\n",
    "\n",
    "print(del1.shape)\n",
    "print(x.T.shape)\n",
    "\n",
    "\n",
    "# calculate all the derivitives for weights\n",
    "dlossdW2 = del2 * a1\n",
    "print(\"dLoss/dW2=\\n\", dlossdW2)\n",
    "dlossdW1 = np.dot(x.T,del1)\n",
    "print(\"dLoss/dW1=\\n\", dlossdW1)\n",
    "\n",
    "\n",
    "dlossB2 = del2\n",
    "dlossB1 = del1\n",
    "\n",
    "# update w1 and w2\n",
    "\n",
    "eta = 0.4\n",
    "\n",
    "print(\"original w1=\\n\", w1)\n",
    "print(\"original w2=\\n\", w2)\n",
    "\n",
    "\n",
    "print(\"original b1=\\n\", b1)\n",
    "print(\"original b2=\\n\", b2)\n",
    "\n",
    "# Update and print w1 and w2 \n",
    "\n",
    "w1 = w1 - eta*dlossdW1\n",
    "print(\"\\nUpdated w1=\\n\",w1)\n",
    "\n",
    "w2 = w2.T - eta*dlossdW2\n",
    "print(\"Updated w2=\\n\",w2)\n",
    "\n",
    "# Update and print b1 and b2\n",
    "b1 = b1 - eta*dlossB1\n",
    "print (\"Updated b1 =\\n\", b1)\n",
    "\n",
    "b2 = b2 - eta*dlossB2\n",
    "print (\"Updated b2 =\\n\", b2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:50%\">\n",
    "    <tr>\n",
    "        <td><center><b>$a_{4}$</b></center></td> \n",
    "        <td><center></b>0.5625</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><center>$W_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.1505\\\\ 0.4518\\\\ \\text{-}0.3509\\\\ \\text{-}0.6131\\\\ \\text{-}0.5198\\\\0.2208\\end{bmatrix}$</center></td> \n",
    "        <td><center>$W_{Output Layer\\, updated} = \\begin{bmatrix}\\text{-}0.7397\\\\ 1.202\\\\ \\text{-}0.3173\\end{bmatrix}$</center></td>     \n",
    "</tr>\n",
    "    <tr>\n",
    "        <td><center>$b_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.4051\\\\ 0.6410\\\\ 0.2323\\end{bmatrix}$</center></td> \n",
    "        <td><center>$b_{Output Layer\\, updated} = \\begin{bmatrix}0.1192\\end{bmatrix}$</center></td>     \n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 [3 marks]**\n",
    "\n",
    "Consider now a more general version of the above neural network architecture containing two inputs, two hidden layers, and two outputs. By using a <i>generalized</i> approach, calculate the results of a <b><i>single iteration</i></b> of <b>Forward and Backward propagation</b> and print <b>1)</b> the predicted values, $a_{8}$ and $a_{9}$, of the network after the first forward pass, and <b>2)</b> the updated values of the parameters <b>weights</b> and <b>biases</b> after the first backward pass. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<img src=\"images/NN_HW2_EX3.png\" style=\"width:1000px;\"><caption><center><b>Figure 3</b>: Neural network with 2 inputs, 2 hidden layers, and 2 outputs.<br><i><u>Note</u>: notations shown differ from your lecture notes.</i></center></caption><br>\n",
    "\n",
    "\n",
    "**Instructions**:\n",
    "\n",
    "- For this exercise, use the <b>activation</b> function $f(z)$ as the **sigmoid** function: $\\sigma(z) = \\frac{1}{ 1 + e^{(-z)}}$\n",
    "\n",
    "    - Note the derivative of the sigmoid function $\\sigma'(z) = \\sigma(z) \\cdot (1 - \\sigma(z))$\n",
    "\n",
    "- Assume the model is fed with only <b>one input data point</b> with the actual ground truth values $y_{1} = 0.25$ and $y_{2} = 0.4$ for the given inputs $x_{1} = 0.1$ and $x_{2} = 0.35$\n",
    "\n",
    "- Use the **Loss** function as: $Loss = \\frac{1}{2} (y - a_{output})^{2}$\n",
    "\n",
    "    - Note the partial derivative of the $Loss$ function $\\frac {\\partial Loss}{\\partial a_{output}} = -(y - a_{output})$\n",
    "\n",
    "- Use the <b>learning rate</b> $\\eta = 0.4$\n",
    "\n",
    "- The initial <b>weights</b>, $w$, are given as:\n",
    "    - $W_{Layer\\, 1:\\, w_{1}\\,\\text{to}\\,w_{6}} = \\begin{bmatrix}0.45 & 0.83 & 0.62 & 0.84 & 0.22 & 0.86\\end{bmatrix}^T$\n",
    "    - $W_{Layer\\, 2:\\, w_{7}\\,\\text{to}\\,w_{18}} = \\begin{bmatrix}0.78 & 1.03 & 0.43 & 0.98 & 0.39 & 0.42 & -0.05 & 1.11 & 0.99 & 0.79 & 0.52 & 1.08\\end{bmatrix}^T$\n",
    "    - $W_{Output\\, Layer:\\, w_{19}\\,\\text{to}\\,w_{26}} = \\begin{bmatrix}0.79 & 0.62 & -0.32 & 0.65 & 0.52 & -0.78 & 0.41 & 0.91\\end{bmatrix}^T$\n",
    "- The initial <b>biases</b>, $b$, are given as:\n",
    "    - $b_{Layer\\, 1:\\, b_{1}\\,\\text{to}\\,b_{3}} = \\begin{bmatrix}0.1 & 1.03 & 0.59\\end{bmatrix}^T$\n",
    "    - $b_{Layer\\, 2:\\, b_{4}\\,\\text{to}\\,b_{7}} = \\begin{bmatrix}0.47 & 0.29 & -0.51 & 1.15\\end{bmatrix}^T$\n",
    "    - $b_{Output\\, Layer:\\, b_{8}\\,\\text{to}\\,b_{9}} = \\begin{bmatrix}0.92 & -0.6\\end{bmatrix}^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "<table style = \"width:70%\">\n",
    "    <tr>\n",
    "        <td colspan=3><center><b>$a_{8}$</b></center></td> \n",
    "        <td colspan=3><center></b>0.9244</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td colspan=3><center><b>$a_{9}$</b></center></td> \n",
    "        <td colspan=3><center></b>0.5956</b></center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td colspan=2><center>$W_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.4499\\\\0.8297\\\\0.6199\\\\0.8398\\\\0.2199\\\\0.8597\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$W_{Layer\\, 2\\, updated} = \\begin{bmatrix}0.7785\\\\1.0280\\\\0.4282\\\\0.9802\\\\0.3903\\\\0.4203\\\\-0.0501\\\\1.1097\\\\0.9897\\\\0.7890\\\\0.5187\\\\1.0788\\end{bmatrix}$</center></td> \n",
    "    <td colspan=2><center>$W_{Output Layer\\, updated} = \\begin{bmatrix}0.7732\\\\0.6046\\\\-0.3339\\\\0.6322\\\\0.5032\\\\-0.7954\\\\0.3960\\\\0.8922\\end{bmatrix}$</center></td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "    <td colspan=2><center>$b_{Layer\\, 1\\, updated} = \\begin{bmatrix}0.0993\\\\1.0294\\\\0.5894\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$b_{Layer\\, 2\\, updated} = \\begin{bmatrix}0.4675\\\\0.2904\\\\-0.5103\\\\1.1484\\end{bmatrix}$</center></td>\n",
    "    <td colspan=2><center>$b_{Output Layer\\, updated} = \\begin{bmatrix}0.9011\\\\-0.6188\\end{bmatrix}$</center></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 4 [2 marks]**\n",
    "\n",
    "Consider now replacing the <b>sigmoid activation function</b> in Exercise 3 with the <b>softmax activation function</b>: $f(z)=\\frac{e^z}{\\sum_i e^z}$ in the <b>output layer</b>, and the <b>ReLU activation function</b>: $f(z)=max(0,z)$ in the two <b>hidden layers</b>. Run your code from Exercise 3 for <b>50 iterations</b> and plot the $Loss$ function against the number of iterations. Compare your results with the <b>sigmoid</b> activation based hidden layers architecture in Exercise 3 and comment on your observations. You may refer to your lecture notebook for examples.\n",
    "\n",
    "<i><b>Hints</b></i>:<br>\n",
    "- Account for the vanishing gradient problem\n",
    "- Recall that the derivative of the <b>softmax</b> function $f(z)=\\frac{e^z}{\\sum_i e^z}$ is $f'(z)=\\frac{e^z}{\\sum_i e^z} - \\frac{(e^z)^2}{(\\sum_i e^z)^2}$\n",
    "- Recall that the derivative of the <b>ReLU</b> function $f(z)=max(0,z)$ is $f(z) = \\left\\{\\begin{matrix}0 \\;\\;\\text{if} \\; z<0\\\\1 \\;\\;\\text{if} \\; z>0\\end{matrix}\\right.$\n",
    "- Sample indicative output plot shown below (your values may differ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ENTER YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output(s)**: \n",
    "\n",
    "<img src=\"images/LossFunction.png\" style=\"width:400px;\"><caption><center><b>Figure 4</b>: Sample indicative plot of $Loss$ function vs. number of iterations (your values may differ)</center></caption><br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
