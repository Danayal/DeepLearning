{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izualkernan/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing backpropagation algorithm \n",
    "\n",
    "COE49412 \n",
    "\n",
    "Imran Zualkernan\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "In this notebook we will implement a backpropagation algorithm from scratch.  The purpose is to understand how the Math translates easily into our code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is John\n"
     ]
    }
   ],
   "source": [
    "# preliminaries\n",
    "# concept of a class in Python\n",
    "# source: https://www.w3schools.com/python/python_classes.asp\n",
    "\n",
    "class Person:\n",
    "  def __init__(self, name, age):\n",
    "    self.name = name\n",
    "    self.age = age\n",
    "\n",
    "  def myfunc(self):\n",
    "    print(\"Hello my name is \" + self.name)\n",
    "\n",
    "# note that there is no new()\n",
    "p1 = Person(\"John\", 36)\n",
    "p1.myfunc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<zip object at 0x118d35280>\n",
      "10 Jenny\n",
      "20 Christy\n",
      "30 Monica\n",
      "hello Vicky\n"
     ]
    }
   ],
   "source": [
    "# preliminaries \n",
    "# concept of zip\n",
    "# source: https://www.w3schools.com/python/ref_func_zip.asp\n",
    "\n",
    "# zip function operates on two list and returns another list of\n",
    "# pairs from each list. \n",
    "\n",
    "a = (\"10\", \"20\", \"30\",\"hello\")\n",
    "b = (\"Jenny\", \"Christy\", \"Monica\", \"Vicky\")\n",
    "\n",
    "x = zip(a, b)\n",
    "print(x)\n",
    "\n",
    "# print using iteration.\n",
    "\n",
    "for i,j in x:\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n",
      "[None, None, None, None, None, None, None, None, None, None]\n"
     ]
    }
   ],
   "source": [
    "# preliminaries\n",
    "\n",
    "# The concept of None\n",
    "# None means uninitalized\n",
    "\n",
    "a = [None]\n",
    "print(a)\n",
    "a= a*10\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.52285838  1.58083976  0.2451904  -0.21308642  0.25781851  0.02110489\n",
      " -0.85522108  0.3963813  -0.91046828 -0.198902  ]\n",
      "[[-0.08480441 -1.48613884  0.17966118 -0.42744942  1.4490254 ]\n",
      " [-1.15388411 -0.69271065 -0.91250861 -1.29791821  0.40915756]\n",
      " [ 0.16373212  1.97138392 -0.34601334 -1.17940855  0.23306396]]\n"
     ]
    }
   ],
   "source": [
    "## randn generates an array of shape (d0, d1, ..., dn), \n",
    "## filled with random floats sampled from a univariate “normal” \n",
    "## (Gaussian) distribution of mean 0 and variance 1\n",
    "\n",
    "# create a random 1x10 array\n",
    "a = np.random.randn(10)\n",
    "print(a)\n",
    "\n",
    "# creat a random 2 x 3 array \n",
    "b = np.random.randn(3,5)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.55988242,  1.54806913,  0.48591684],\n",
       "        [ 0.05058884, -1.55617956,  0.72691948]]),\n",
       " array([[-0.3426209 ,  0.16895151,  1.16732847,  0.34590021,  1.59205969,\n",
       "          0.90587903,  1.11517783],\n",
       "        [ 0.69075162, -0.37429602, -0.01105215,  0.95360165,  1.13605643,\n",
       "          0.08898351, -0.34250533],\n",
       "        [ 0.02278797,  0.45235404,  0.50951145,  0.28988815, -0.78177941,\n",
       "         -1.48944157,  2.08795471],\n",
       "        [-1.3638775 , -0.99256073,  1.23744338, -1.05376303,  0.76483575,\n",
       "          0.80283364,  0.54704587]])]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using append to hold arrays in a list\n",
    "# We can use append to build up a neural network\n",
    "# layer by layer.\n",
    "\n",
    "# weights is a list of arrays\n",
    "weights = []\n",
    "\n",
    "# create two 2 x 3 arrays and add them to the weights list\n",
    "weights.append(np.random.randn(2,3))\n",
    "weights.append(np.random.randn(3,7))\n",
    "\n",
    "# print weights\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function f.<locals>.<lambda> at 0x108bb34d0>\n",
      "<function f.<locals>.<lambda> at 0x10a143320>\n",
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Using and returning annoymous functions\n",
    "# We will use this idea to supply different activation \n",
    "# functions.\n",
    "\n",
    "# Example: Write a function f that returns a function \n",
    "# for adding or subtracting 1 from x\n",
    "# lambda x is like an anonymous function in java \n",
    "# originally derived from lambda calculus.\n",
    "\n",
    "def f(op):\n",
    "  if(op == '+'):\n",
    "    return lambda x : x+1\n",
    "  elif(op == '-'):\n",
    "    return lambda x : x-1\n",
    "        \n",
    "# calling f will return a function called add\n",
    "add = f('+')\n",
    "sub = f('-')\n",
    "\n",
    "# add is a function\n",
    "print(add)\n",
    "print(sub)\n",
    "\n",
    "# now we can call the function normally\n",
    "print(add(3))\n",
    "print(sub(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "3\n",
      "5\n",
      "5\n",
      "3\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Using reversed to reverse a list\n",
    "# reversed returns an iterator that \n",
    "# goes over the list backwards\n",
    "# We will use this idea in back propagation.\n",
    "\n",
    "a = [1,3,5]\n",
    "\n",
    "# forward\n",
    "for x in a:\n",
    "    print(x)\n",
    "\n",
    "# backwards\n",
    "for x in (reversed(a)):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 foo\n",
      "1 bar\n",
      "2 coo\n",
      "3 fee\n",
      "4 foo\n",
      "5 fum\n"
     ]
    }
   ],
   "source": [
    "# Using enumerate to use the index of a list\n",
    "\n",
    "x = [\"foo\", \"bar\", \"coo\", \"fee\", \"foo\", \"fum\"]\n",
    "\n",
    "e = enumerate(x)\n",
    "\n",
    "# enumerate has pairs like (counter, value)\n",
    "\n",
    "for counter, value in e:\n",
    "    print(counter, value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing backpropagation\n",
    "## Source: \n",
    "## https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "## comments added to the original code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    # The constructor takes \n",
    "    # 1. layers representing the number of nodes\n",
    "    # 2. activations representing the activation functions to choose in\n",
    "    #    each layer. \n",
    "    \n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        \n",
    "        # check to make sure that no. of layers is one more than \n",
    "        # no. of activation functions because the input layer \n",
    "        # has no activation function.\n",
    "        \n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        \n",
    "        # define the local variables layers and activations\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        \n",
    "        # initialize weights and biases as two lists to hold\n",
    "        # weights and biases for each layer\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # create random weights for biases and weights\n",
    "        # for each of the layes.\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
    "    \n",
    "    # do feedforward for x\n",
    "    # where x is an input\n",
    "    \n",
    "    # return a list of a's and z's as expected.\n",
    "    # a's and z's are layer by layer\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # make a copy of x\n",
    "        a = np.copy(x)\n",
    "        \n",
    "        # this variable will contain all the z's\n",
    "        z_s = []\n",
    "        \n",
    "        # this variable will contain all the a's\n",
    "        # the output of the input layer is simply x which is the\n",
    "        # input. So we initialize the a_s to contain a. \n",
    "        a_s = [a]\n",
    "        \n",
    "        # for each layer do\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            # retrieve the appropriate activation function\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            \n",
    "            # create z_s by z = w.a + b for each layer\n",
    "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "            \n",
    "            # create z_a by a = f(row) -- \n",
    "            # note that we apply it to the last element only \n",
    "            # by using the -1 notation. We only want to apply\n",
    "            # the activation function to the last layer just \n",
    "            # added.\n",
    "            \n",
    "            a = activation_function(z_s[-1])\n",
    "            \n",
    "            # keep track of the new activation or a_s \n",
    "            a_s.append(a)\n",
    "            \n",
    "            # return both z_s and a_s \n",
    "            # we will have z_s and a_s for each layer \n",
    "        return (z_s, a_s)\n",
    "\n",
    "    \n",
    "    # takes the y -- the actual answer, a's and z's and \n",
    "    # calculates the dLoss/dw and dLoss/db\n",
    "    \n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "        \n",
    "        # initialize list of dLoss/dw and dLoss/db\n",
    "        dw = []  # dLoss/dw\n",
    "        db = []  # dLoss/db\n",
    "        \n",
    "        # create an empty list of deltas, one for each weight\n",
    "        deltas = [None] * len(self.weights)  # delta = dLoss/dz  known as error for each layer\n",
    "        \n",
    "        \n",
    "        # start from the back and insert the last layer error\n",
    "        # based on the square loss function. Note -1 is used to \n",
    "        # fill things from the back of the list \n",
    "        # also note that we need to use the derivative function \n",
    "        # for the activation function.\n",
    "        # note that we do not need to use the 2 in the loss function derivation\n",
    "        \n",
    "        # again note this is for the last layer only!\n",
    "        \n",
    "        deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "        \n",
    "        \n",
    "        # Perform BackPropagation\n",
    "        \n",
    "        # for the rest of the deltas, go in reverse order\n",
    "        for i in reversed(range(len(deltas)-1)):\n",
    "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        \n",
    "        # now we need to update the weights based on the calculated\n",
    "        # deltas\n",
    "        \n",
    "        #now we will determine the batch size from the first dimension \n",
    "        #of shape of y. We simply want to see how many test cases are there\n",
    "        #for example there may be 10 y's; one for each x. \n",
    "        \n",
    "        batch_size = y.shape[1]\n",
    "        \n",
    "        # determine the two derivatives by taking \n",
    "        # the average according to batch sizes \n",
    "    \n",
    "        \n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        \n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "        return dw, db\n",
    "\n",
    "    \n",
    "    # Now we will write the main training function that uses\n",
    "    # feedforward and backpropagation many times (called epochs)\n",
    "    # lr (learning rate) is the eta in our equations.\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "    \n",
    "    # update weights and biases based on the output\n",
    "    # for the number of epochs\n",
    "    \n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            \n",
    "            # Do the training in batches\n",
    "            # each batch is a subset of the original \n",
    "            # data \n",
    "            \n",
    "            while(i<len(y)):\n",
    "                \n",
    "                # extract a batch\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                # update i for the next batches\n",
    "                i = i+batch_size\n",
    "                \n",
    "                # do the feedforward for the batch and update the weights\n",
    "                # based on the average loss for each weight for the whole\n",
    "                # batch.\n",
    "                \n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                \n",
    "                # do the back propagation \n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                \n",
    "                \n",
    "                # update the weights for each pair of weights and dw\n",
    "                # and biases and db\n",
    "                \n",
    "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "                \n",
    "                # print the loss using a built in function \n",
    "                # to calculate the loss\n",
    "                \n",
    "                print(\"loss = \", np.linalg.norm(a_s[-1]-y_batch) )\n",
    "    \n",
    "    \n",
    "    # This function is being used to return an activation function \n",
    "    # depending on its weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "    \n",
    "    # This function returns the derivative of a function depending\n",
    "    # on its name.\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 1]\n",
      "['sigmoid', 'sigmoid']\n",
      "[array([[-1.11092136, -1.22907365],\n",
      "       [-0.95507278,  0.0576407 ],\n",
      "       [ 0.49999654, -1.91488116]]), array([[ 2.11913924,  1.74288539, -0.49865427]])]\n",
      "[array([[-0.01969041],\n",
      "       [ 0.14240766],\n",
      "       [ 0.02741788]]), array([[0.53355539]])]\n"
     ]
    }
   ],
   "source": [
    "# Let us see what the constructor does.\n",
    "# for a simple 3 layer network\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([2, 3, 1],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "print(nn.layers)\n",
    "print(nn.activations)\n",
    "\n",
    "print(nn.weights)\n",
    "print(nn.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 1]\n",
      "['sigmoid', 'sigmoid', 'sigmoid']\n",
      "[array([[0.61486887],\n",
      "       [0.11471677]]), array([[ 2.14108144, -0.98368127],\n",
      "       [ 0.21920002, -1.4756294 ],\n",
      "       [-0.04453245,  0.42606575]]), array([[ 0.35117885, -0.47966615, -0.36995642]])]\n",
      "[array([[ 0.98017579],\n",
      "       [-0.3117457 ]]), array([[ 0.81209901],\n",
      "       [-0.32702442],\n",
      "       [-0.46760101]]), array([[-0.34314965]])]\n"
     ]
    }
   ],
   "source": [
    "# Let us see what the constructor does.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "nn = NeuralNetwork([1, 2,3,1],activations=['sigmoid', 'sigmoid','sigmoid'])\n",
    "\n",
    "\n",
    "print(nn.layers)\n",
    "print(nn.activations)\n",
    "\n",
    "# should see the arrays as \n",
    "# 1 x 2\n",
    "# 2 x 3\n",
    "# 3 x 1 \n",
    "\n",
    "# but this implementation does it backwards\n",
    "# as long as we are consistent the math is \n",
    "# the same.\n",
    "\n",
    "# 2 x 1\n",
    "# 3 x 2 \n",
    "# 1 x 3\n",
    "\n",
    "print(nn.weights)\n",
    "print(nn.biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights= [array([[ 0.23708622, -0.89761943],\n",
      "       [ 0.50739804,  2.07836043],\n",
      "       [-1.08372785,  1.72855885]]), array([[-0.77785863, -1.21570196,  0.02441235]])]\n",
      "z_s= [array([[-1.55671869],\n",
      "       [ 2.23491266],\n",
      "       [ 0.09842918]]), array([[-0.09926471]])]\n",
      "a_s= [array([[2],\n",
      "       [3]]), array([[0.02682335],\n",
      "       [0.99902325],\n",
      "       [0.88745298]]), array([[0.37528392]])]\n"
     ]
    }
   ],
   "source": [
    "# Let us try feedforward on one simple output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([2, 3, 1],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "x = [[2],[3]] # 2 x 1 \n",
    "## Let us do one feedforward\n",
    "## Remember input is 1 row\n",
    "a, a = nn.feedforward(x)\n",
    "\n",
    "print(\"weights=\", nn.weights)  ## 3 x 2\n",
    "\n",
    "print(\"z_s=\",z)\n",
    "print(\"a_s=\",a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(1, 4)\n",
      "a= [array([[0.51407879, 0.52351973, 0.39458503, 0.53413611],\n",
      "       [0.63517536, 0.44564836, 0.66887408, 0.15559223]]), array([[0.65620179, 0.65107037, 0.64649248, 0.64279984],\n",
      "       [0.65810307, 0.5844534 , 0.68794488, 0.46592308],\n",
      "       [0.27775991, 0.2625793 , 0.3255199 , 0.24167572]]), array([[0.66862352, 0.61709145, 0.6921017 , 0.52901943]])]\n",
      "z= [array([[ 0.64641347,  0.62374747,  0.60365681,  0.58753707],\n",
      "       [ 0.65485225,  0.34108231,  0.79052889, -0.1365193 ],\n",
      "       [-0.95560051, -1.03260542, -0.72851857, -1.14351429]]), array([[0.70196602, 0.47722105, 0.8099634 , 0.11620833]])]\n",
      "z (3, 4)\n",
      "z (1, 4)\n",
      "a (2, 4)\n",
      "a (3, 4)\n",
      "a (1, 4)\n",
      "dw= [array([[-0.03747772, -0.02836124],\n",
      "       [ 0.15832858,  0.11489893],\n",
      "       [ 0.00807241,  0.00646125]]), array([[0.28369605, 0.2451438 , 0.11882376]])]\n",
      "db= [array([[-0.07612673],\n",
      "       [ 0.31923822],\n",
      "       [ 0.01655592]]), array([[0.43920422]])]\n"
     ]
    }
   ],
   "source": [
    "# Let us try feedforward on one simple output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "    \n",
    "nn = NeuralNetwork([2, 3, 1],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "# note that x is a 2 x 4 matrix which means we have four \n",
    "# data points. Each column represents a data point\n",
    "\n",
    "X = np.random.rand(2,4)\n",
    "\n",
    "# note that y is 1 x 4 matrix where each element (column) is the \n",
    "# output for the correponding data point\n",
    "\n",
    "y = np.array([[1,2,3,4]])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "z, a = nn.feedforward(X)\n",
    "\n",
    "# print a and z\n",
    "print(\"a=\",a)\n",
    "print(\"z=\",z)\n",
    "\n",
    "for i in z:\n",
    "    print(\"z\",i.shape)\n",
    "    \n",
    "for i in a: \n",
    "    print(\"a\",i.shape)\n",
    "\n",
    "dw, db = nn.backpropagation(y, z,a)\n",
    "\n",
    "print(\"dw=\",dw)\n",
    "print(\"db=\",db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n",
      "(2, 4)\n",
      "a= [array([[0.91629691, 0.06980572, 0.89830184, 0.40632016],\n",
      "       [0.96121621, 0.24113957, 0.26759889, 0.81885333]]), array([[0.26141919, 0.36607213, 0.3374579 , 0.28995226],\n",
      "       [0.10979779, 0.15260962, 0.13437052, 0.12345547],\n",
      "       [0.04139104, 0.29545233, 0.21250782, 0.073139  ]]), array([[0.48561028, 0.40843547, 0.43269112, 0.4664271 ],\n",
      "       [0.29543567, 0.32284437, 0.31149563, 0.29691556]])]\n",
      "z= [array([[-1.03860529, -0.54910477, -0.67464334, -0.89561594],\n",
      "       [-2.09280826, -1.71427835, -1.86285592, -1.96010698],\n",
      "       [-3.14241868, -0.8690485 , -1.30987468, -2.53944186]]), array([[-0.0575748 , -0.37043686, -0.2708798 , -0.13449398],\n",
      "       [-0.86912855, -0.74073075, -0.79313636, -0.86202922]])]\n",
      "z (3, 4)\n",
      "z (2, 4)\n",
      "a (2, 4)\n",
      "a (3, 4)\n",
      "a (2, 4)\n",
      "dw= [array([[-0.2656305 , -0.26434214],\n",
      "       [ 0.03480847,  0.03359383],\n",
      "       [ 0.05266027,  0.04274685]]), array([[0.16050343, 0.06651036, 0.07929364],\n",
      "       [0.41516778, 0.1720505 , 0.20741747]])]\n",
      "db= [array([[-0.49339452],\n",
      "       [ 0.06222603],\n",
      "       [ 0.10975507]]), array([[0.50566118],\n",
      "       [1.31572891]])]\n"
     ]
    }
   ],
   "source": [
    "# Let us try feedforward on more complex output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the NN has two inputs and two output neurons. So x is <x1, x2> and \n",
    "# y is <y1, y2>\n",
    "\n",
    "nn = NeuralNetwork([2, 3, 2],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "# note that x is a 2 x 4 matrix which means we have four \n",
    "# data points. Each column represents a data point\n",
    "\n",
    "X = np.random.rand(2,4)\n",
    "\n",
    "# note that y is 2 x 4 matrix where each column is the \n",
    "# output for the correponding data point\n",
    "\n",
    "y = np.array([[1,2,3,4], [5,6,7,8]])\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "z, a = nn.feedforward(X)\n",
    "\n",
    "# print a and z\n",
    "print(\"a=\",a)\n",
    "print(\"z=\",z)\n",
    "\n",
    "for i in z:\n",
    "    print(\"z\",i.shape)\n",
    "    \n",
    "for i in a: \n",
    "    print(\"a\",i.shape)\n",
    "\n",
    "dw, db = nn.backpropagation(y, z,a)\n",
    "\n",
    "print(\"dw=\",dw)\n",
    "print(\"db=\",db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1000)\n",
      "(1, 1000)\n",
      "loss =  22.32041619921443\n",
      "loss =  22.320042667045914\n",
      "loss =  22.31966733605442\n",
      "loss =  22.31929020084415\n",
      "loss =  22.318911256066304\n",
      "loss =  22.318530496419648\n",
      "loss =  22.31814791665111\n",
      "loss =  22.31776351155632\n",
      "loss =  22.317377275980096\n",
      "loss =  22.316989204816995\n",
      "loss =  22.316599293011706\n",
      "loss =  22.31620753555953\n",
      "loss =  22.315813927506774\n",
      "loss =  22.315418463951094\n",
      "loss =  22.315021140041864\n",
      "loss =  22.314621950980456\n",
      "loss =  22.314220892020504\n",
      "loss =  22.313817958468146\n",
      "loss =  22.31341314568219\n",
      "loss =  22.31300644907428\n",
      "loss =  22.312597864108994\n",
      "loss =  22.312187386303897\n",
      "loss =  22.311775011229578\n",
      "loss =  22.3113607345096\n",
      "loss =  22.31094455182044\n",
      "loss =  22.310526458891356\n",
      "loss =  22.31010645150421\n",
      "loss =  22.309684525493246\n",
      "loss =  22.309260676744792\n",
      "loss =  22.308834901196942\n",
      "loss =  22.30840719483916\n",
      "loss =  22.307977553711808\n",
      "loss =  22.307545973905665\n",
      "loss =  22.30711245156132\n",
      "loss =  22.306676982868556\n",
      "loss =  22.306239564065653\n",
      "loss =  22.305800191438603\n",
      "loss =  22.30535886132029\n",
      "loss =  22.30491557008958\n",
      "loss =  22.304470314170338\n",
      "loss =  22.304023090030398\n",
      "loss =  22.303573894180424\n",
      "loss =  22.30312272317273\n",
      "loss =  22.302669573599985\n",
      "loss =  22.302214442093877\n",
      "loss =  22.301757325323667\n",
      "loss =  22.301298219994674\n",
      "loss =  22.3008371228467\n",
      "loss =  22.300374030652304\n",
      "loss =  22.299908940215065\n",
      "loss =  22.29944184836771\n",
      "loss =  22.298972751970172\n",
      "loss =  22.298501647907532\n",
      "loss =  22.29802853308792\n",
      "loss =  22.29755340444026\n",
      "loss =  22.29707625891197\n",
      "loss =  22.296597093466524\n",
      "loss =  22.296115905080956\n",
      "loss =  22.295632690743233\n",
      "loss =  22.295147447449544\n",
      "loss =  22.294660172201485\n",
      "loss =  22.294170862003128\n",
      "loss =  22.293679513858002\n",
      "loss =  22.293186124765963\n",
      "loss =  22.29269069171995\n",
      "loss =  22.292193211702646\n",
      "loss =  22.291693681683007\n",
      "loss =  22.291192098612715\n",
      "loss =  22.29068845942248\n",
      "loss =  22.290182761018265\n",
      "loss =  22.289675000277377\n",
      "loss =  22.289165174044445\n",
      "loss =  22.28865327912729\n",
      "loss =  22.288139312292664\n",
      "loss =  22.28762327026191\n",
      "loss =  22.287105149706445\n",
      "loss =  22.286584947243167\n",
      "loss =  22.286062659429753\n",
      "loss =  22.285538282759774\n",
      "loss =  22.285011813657768\n",
      "loss =  22.284483248474142\n",
      "loss =  22.28395258347995\n",
      "loss =  22.283419814861592\n",
      "loss =  22.282884938715338\n",
      "loss =  22.282347951041743\n",
      "loss =  22.281808847739995\n",
      "loss =  22.28126762460203\n",
      "loss =  22.280724277306618\n",
      "loss =  22.28017880141329\n",
      "loss =  22.279631192356117\n",
      "loss =  22.27908144543741\n",
      "loss =  22.278529555821247\n",
      "loss =  22.27797551852691\n",
      "loss =  22.27741932842219\n",
      "loss =  22.276860980216536\n",
      "loss =  22.276300468454107\n",
      "loss =  22.275737787506706\n",
      "loss =  22.27517293156656\n",
      "loss =  22.274605894638974\n",
      "loss =  22.274036670534894\n",
      "loss =  22.273465252863303\n",
      "loss =  22.272891635023502\n",
      "loss =  22.272315810197284\n",
      "loss =  22.271737771340955\n",
      "loss =  22.27115751117724\n",
      "loss =  22.27057502218706\n",
      "loss =  22.269990296601193\n",
      "loss =  22.26940332639178\n",
      "loss =  22.26881410326375\n",
      "loss =  22.268222618646053\n",
      "loss =  22.267628863682845\n",
      "loss =  22.267032829224462\n",
      "loss =  22.26643450581835\n",
      "loss =  22.265833883699795\n",
      "loss =  22.26523095278257\n",
      "loss =  22.26462570264944\n",
      "loss =  22.264018122542538\n",
      "loss =  22.263408201353613\n",
      "loss =  22.262795927614142\n",
      "loss =  22.262181289485333\n",
      "loss =  22.26156427474798\n",
      "loss =  22.260944870792173\n",
      "loss =  22.260323064606922\n",
      "loss =  22.259698842769605\n",
      "loss =  22.2590721914353\n",
      "loss =  22.25844309632598\n",
      "loss =  22.25781154271958\n",
      "loss =  22.2571775154389\n",
      "loss =  22.25654099884042\n",
      "loss =  22.255901976802924\n",
      "loss =  22.255260432716\n",
      "loss =  22.254616349468417\n",
      "loss =  22.253969709436326\n",
      "loss =  22.253320494471335\n",
      "loss =  22.252668685888416\n",
      "loss =  22.252014264453692\n",
      "loss =  22.25135721037202\n",
      "loss =  22.250697503274463\n",
      "loss =  22.250035122205578\n",
      "loss =  22.249370045610547\n",
      "loss =  22.248702251322136\n",
      "loss =  22.248031716547484\n",
      "loss =  22.247358417854723\n",
      "loss =  22.246682331159413\n",
      "loss =  22.246003431710797\n",
      "loss =  22.245321694077862\n",
      "loss =  22.24463709213523\n",
      "loss =  22.243949599048793\n",
      "loss =  22.243259187261234\n",
      "loss =  22.242565828477254\n",
      "loss =  22.241869493648622\n",
      "loss =  22.24117015295902\n",
      "loss =  22.240467775808604\n",
      "loss =  22.2397623307984\n",
      "loss =  22.239053785714397\n",
      "loss =  22.238342107511414\n",
      "loss =  22.237627262296716\n",
      "loss =  22.236909215313336\n",
      "loss =  22.236187930923162\n",
      "loss =  22.235463372589674\n",
      "loss =  22.234735502860453\n",
      "loss =  22.234004283349343\n",
      "loss =  22.233269674718294\n",
      "loss =  22.2325316366589\n",
      "loss =  22.231790127873595\n",
      "loss =  22.231045106056445\n",
      "loss =  22.230296527873662\n",
      "loss =  22.229544348943662\n",
      "loss =  22.22878852381677\n",
      "loss =  22.22802900595451\n",
      "loss =  22.227265747708476\n",
      "loss =  22.22649870029874\n",
      "loss =  22.225727813791863\n",
      "loss =  22.224953037078354\n",
      "loss =  22.22417431784973\n",
      "loss =  22.223391602574992\n",
      "loss =  22.22260483647662\n",
      "loss =  22.22181396350601\n",
      "loss =  22.221018926318337\n",
      "loss =  22.220219666246848\n",
      "loss =  22.219416123276517\n",
      "loss =  22.218608236017097\n",
      "loss =  22.217795941675497\n",
      "loss =  22.216979176027483\n",
      "loss =  22.21615787338865\n",
      "loss =  22.2153319665847\n",
      "loss =  22.214501386920926\n",
      "loss =  22.2136660641509\n",
      "loss =  22.212825926444367\n",
      "loss =  22.21198090035425\n",
      "loss =  22.2111309107828\n",
      "loss =  22.210275880946792\n",
      "loss =  22.209415732341817\n",
      "loss =  22.208550384705514\n",
      "loss =  22.207679755979832\n",
      "loss =  22.206803762272187\n",
      "loss =  22.20592231781552\n",
      "loss =  22.205035334927206\n",
      "loss =  22.204142723966754\n",
      "loss =  22.203244393292273\n",
      "loss =  22.202340249215652\n",
      "loss =  22.201430195956373\n",
      "loss =  22.20051413559397\n",
      "loss =  22.199591968019003\n",
      "loss =  22.19866359088255\n",
      "loss =  22.197728899544146\n",
      "loss =  22.19678778701807\n",
      "loss =  22.195840143917987\n",
      "loss =  22.194885858399804\n",
      "loss =  22.193924816102726\n",
      "loss =  22.192956900088408\n",
      "loss =  22.191981990778178\n",
      "loss =  22.190999965888174\n",
      "loss =  22.19001070036237\n",
      "loss =  22.18901406630343\n",
      "loss =  22.188009932901213\n",
      "loss =  22.186998166358936\n",
      "loss =  22.185978629816848\n",
      "loss =  22.184951183273302\n",
      "loss =  22.183915683503177\n",
      "loss =  22.182871983973474\n",
      "loss =  22.18181993475604\n",
      "loss =  22.180759382437245\n",
      "loss =  22.179690170024525\n",
      "loss =  22.17861213684964\n",
      "loss =  22.177525118468544\n",
      "loss =  22.176428946557667\n",
      "loss =  22.17532344880655\n",
      "loss =  22.17420844880658\n",
      "loss =  22.173083765935743\n",
      "loss =  22.171949215239174\n",
      "loss =  22.170804607305385\n",
      "loss =  22.169649748137903\n",
      "loss =  22.168484439022215\n",
      "loss =  22.167308476387742\n",
      "loss =  22.16612165166469\n",
      "loss =  22.164923751135504\n",
      "loss =  22.16371455578072\n",
      "loss =  22.162493841118984\n",
      "loss =  22.16126137704093\n",
      "loss =  22.160016927636715\n",
      "loss =  22.158760251016844\n",
      "loss =  22.15749109912609\n",
      "loss =  22.156209217550114\n",
      "loss =  22.154914345314474\n",
      "loss =  22.153606214675705\n",
      "loss =  22.152284550904103\n",
      "loss =  22.1509490720578\n",
      "loss =  22.14959948874775\n",
      "loss =  22.14823550389322\n",
      "loss =  22.14685681246731\n",
      "loss =  22.14546310123205\n",
      "loss =  22.144054048462586\n",
      "loss =  22.142629323659946\n",
      "loss =  22.141188587251772\n",
      "loss =  22.13973149028055\n",
      "loss =  22.13825767407863\n",
      "loss =  22.136766769929476\n",
      "loss =  22.135258398714377\n",
      "loss =  22.133732170544\n",
      "loss =  22.132187684373953\n",
      "loss =  22.13062452760363\n",
      "loss =  22.1290422756574\n",
      "loss =  22.127440491547375\n",
      "loss =  22.125818725416682\n",
      "loss =  22.12417651406234\n",
      "loss =  22.122513380436665\n",
      "loss =  22.120828833126012\n",
      "loss =  22.119122365805836\n",
      "loss =  22.117393456670598\n",
      "loss =  22.115641567837404\n",
      "loss =  22.11386614472182\n",
      "loss =  22.112066615384435\n",
      "loss =  22.1102423898466\n",
      "loss =  22.108392859373595\n",
      "loss =  22.10651739572352\n",
      "loss =  22.104615350359985\n",
      "loss =  22.102686053626538\n",
      "loss =  22.100728813880796\n",
      "loss =  22.09874291658588\n",
      "loss =  22.09672762335685\n",
      "loss =  22.094682170959477\n",
      "loss =  22.09260577025866\n",
      "loss =  22.09049760511356\n",
      "loss =  22.0883568312163\n",
      "loss =  22.086182574871\n",
      "loss =  22.08397393170953\n",
      "loss =  22.08172996534026\n",
      "loss =  22.079449705925725\n",
      "loss =  22.07713214868503\n",
      "loss =  22.07477625231626\n",
      "loss =  22.072380937334064\n",
      "loss =  22.06994508431723\n",
      "loss =  22.067467532060522\n",
      "loss =  22.064947075624904\n",
      "loss =  22.062382464279644\n",
      "loss =  22.059772399329518\n",
      "loss =  22.057115531819658\n",
      "loss =  22.054410460110265\n",
      "loss =  22.051655727312646\n",
      "loss =  22.048849818577573\n",
      "loss =  22.0459911582262\n",
      "loss =  22.043078106713125\n",
      "loss =  22.04010895741037\n",
      "loss =  22.037081933200245\n",
      "loss =  22.033995182864146\n",
      "loss =  22.03084677725339\n",
      "loss =  22.02763470522709\n",
      "loss =  22.02435686934101\n",
      "loss =  22.021011081269982\n",
      "loss =  22.01759505694535\n",
      "loss =  22.014106411387196\n",
      "loss =  22.01054265320975\n",
      "loss =  22.006901178776623\n",
      "loss =  22.00317926598065\n",
      "loss =  21.99937406762119\n",
      "loss =  21.995482604349533\n",
      "loss =  21.991501757150836\n",
      "loss =  21.98742825932831\n",
      "loss =  21.983258687952915\n",
      "loss =  21.978989454738585\n",
      "loss =  21.974616796300023\n",
      "loss =  21.970136763746552\n",
      "loss =  21.965545211561768\n",
      "loss =  21.960837785714723\n",
      "loss =  21.956009910944\n",
      "loss =  21.95105677715128\n",
      "loss =  21.945973324836025\n",
      "loss =  21.94075422949726\n",
      "loss =  21.93539388492276\n",
      "loss =  21.92988638527945\n",
      "loss =  21.92422550591214\n",
      "loss =  21.91840468275058\n",
      "loss =  21.91241699021692\n",
      "loss =  21.906255117517684\n",
      "loss =  21.89991134319569\n",
      "loss =  21.893377507808225\n",
      "loss =  21.886644984588408\n",
      "loss =  21.879704647936883\n",
      "loss =  21.872546839580863\n",
      "loss =  21.865161332227583\n",
      "loss =  21.857537290529006\n",
      "loss =  21.84966322916502\n",
      "loss =  21.841526967843187\n",
      "loss =  21.833115583005124\n",
      "loss =  21.824415356023273\n",
      "loss =  21.81541171766751\n",
      "loss =  21.80608918862043\n",
      "loss =  21.79643131582347\n",
      "loss =  21.78642060444542\n",
      "loss =  21.776038445281802\n",
      "loss =  21.765265037420562\n",
      "loss =  21.75407930604926\n",
      "loss =  21.74245881533512\n",
      "loss =  21.7303796763866\n",
      "loss =  21.717816450408662\n",
      "loss =  21.70474204730122\n",
      "loss =  21.691127620129325\n",
      "loss =  21.67694245612526\n",
      "loss =  21.662153865179647\n",
      "loss =  21.64672706715665\n",
      "loss =  21.63062507984622\n",
      "loss =  21.613808609967034\n",
      "loss =  21.596235950384585\n",
      "loss =  21.577862887642066\n",
      "loss =  21.558642625054517\n",
      "loss =  21.53852572803298\n",
      "loss =  21.5174601000328\n",
      "loss =  21.49539099961172\n",
      "loss =  21.472261111593454\n",
      "loss =  21.448010688313698\n",
      "loss =  21.42257778042082\n",
      "loss =  21.3958985807377\n",
      "loss =  21.36790790925099\n",
      "loss =  21.338539872308587\n",
      "loss =  21.307728734409025\n",
      "loss =  21.27541004625364\n",
      "loss =  21.241522077497894\n",
      "loss =  21.206007606100616\n",
      "loss =  21.16881611718074\n",
      "loss =  21.1299064612388\n",
      "loss =  21.08925001232627\n",
      "loss =  21.046834348500205\n",
      "loss =  21.002667446399837\n",
      "loss =  20.95678233543986\n",
      "loss =  20.909242091563343\n",
      "loss =  20.860144963518312\n",
      "loss =  20.809629316604113\n",
      "loss =  20.75787795477306\n",
      "loss =  20.70512125384373\n",
      "loss =  20.651638427538806\n",
      "loss =  20.59775618544958\n",
      "loss =  20.54384406766359\n",
      "loss =  20.490305896741386\n",
      "loss =  20.43756710642692\n",
      "loss =  20.38605819385172\n",
      "loss =  20.33619515959875\n",
      "loss =  20.288358453962267\n",
      "loss =  20.242872494457803\n",
      "loss =  20.199988095244226\n",
      "loss =  20.159870020718095\n",
      "loss =  20.12259129836701\n",
      "loss =  20.08813498058663\n",
      "loss =  20.056402930728353\n",
      "loss =  20.027230186229374\n",
      "loss =  20.00040275768925\n",
      "loss =  19.975676490389425\n",
      "loss =  19.95279483850909\n",
      "loss =  19.931503957671193\n",
      "loss =  19.911564224099397\n",
      "loss =  19.89275796169903\n",
      "loss =  19.87489368002728\n",
      "loss =  19.857807445803722\n",
      "loss =  19.841362138108455\n",
      "loss =  19.825445318554294\n",
      "loss =  19.809966339687236\n",
      "loss =  19.79485316879827\n",
      "loss =  19.780049256998616\n",
      "loss =  19.76551065612044\n",
      "loss =  19.75120348748695\n",
      "loss =  19.737101797229542\n",
      "loss =  19.723185788517807\n",
      "loss =  19.70944039605843\n",
      "loss =  19.695854156819795\n",
      "loss =  19.682418328354224\n",
      "loss =  19.669126208676445\n",
      "loss =  19.65597261686693\n",
      "loss =  19.64295349974822\n",
      "loss =  19.630065636145282\n",
      "loss =  19.617306415860867\n",
      "loss =  19.604673675345886\n",
      "loss =  19.592165576073015\n",
      "loss =  19.57978051487723\n",
      "loss =  19.567517058104126\n",
      "loss =  19.555373893413826\n",
      "loss =  19.543349794631695\n",
      "loss =  19.531443596211577\n",
      "loss =  19.51965417476363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  19.507980435763283\n",
      "loss =  19.496421304052976\n",
      "loss =  19.48497571711572\n",
      "loss =  19.473642620371045\n",
      "loss =  19.46242096394407\n",
      "loss =  19.45130970050527\n",
      "loss =  19.44030778388671\n",
      "loss =  19.429414168259303\n",
      "loss =  19.41862780771359\n",
      "loss =  19.40794765612893\n",
      "loss =  19.397372667246746\n",
      "loss =  19.386901794886263\n",
      "loss =  19.37653399325761\n",
      "loss =  19.366268217339357\n",
      "loss =  19.356103423296325\n",
      "loss =  19.346038568920203\n",
      "loss =  19.336072614080063\n",
      "loss =  19.3262045211736\n",
      "loss =  19.316433255572377\n",
      "loss =  19.306757786056327\n",
      "loss =  19.297177085234214\n",
      "loss =  19.28769012994775\n",
      "loss =  19.278295901657863\n",
      "loss =  19.26899338681223\n",
      "loss =  19.259781577193564\n",
      "loss =  19.25065947024852\n",
      "loss =  19.24162606939726\n",
      "loss =  19.232680384323906\n",
      "loss =  19.22382143124837\n",
      "loss =  19.215048233179846\n",
      "loss =  19.206359820152652\n",
      "loss =  19.197755229444923\n",
      "loss =  19.189233505780766\n",
      "loss =  19.180793701516507\n",
      "loss =  19.172434876811653\n",
      "loss =  19.164156099785213\n",
      "loss =  19.155956446657985\n",
      "loss =  19.14783500188141\n",
      "loss =  19.13979085825369\n",
      "loss =  19.1318231170236\n",
      "loss =  19.123930887982752\n",
      "loss =  19.116113289546714\n",
      "loss =  19.108369448825655\n",
      "loss =  19.10069850168493\n",
      "loss =  19.09309959279622\n",
      "loss =  19.085571875679612\n",
      "loss =  19.078114512737162\n",
      "loss =  19.070726675278394\n",
      "loss =  19.063407543538105\n",
      "loss =  19.05615630668698\n",
      "loss =  19.04897216283534\n",
      "loss =  19.041854319030495\n",
      "loss =  19.034801991247964\n",
      "loss =  19.027814404377022\n",
      "loss =  19.02089079220084\n",
      "loss =  19.01403039737155\n",
      "loss =  19.007232471380608\n",
      "loss =  19.00049627452466\n",
      "loss =  18.993821075867242\n",
      "loss =  18.987206153196585\n",
      "loss =  18.980650792979798\n",
      "loss =  18.974154290313585\n",
      "loss =  18.96771594887188\n",
      "loss =  18.96133508085047\n",
      "loss =  18.95501100690894\n",
      "loss =  18.948743056110033\n",
      "loss =  18.942530565856753\n",
      "loss =  18.936372881827243\n",
      "loss =  18.930269357907733\n",
      "loss =  18.924219356123658\n",
      "loss =  18.918222246569126\n",
      "loss =  18.912277407334877\n",
      "loss =  18.906384224434905\n",
      "loss =  18.900542091731833\n",
      "loss =  18.894750410861167\n",
      "loss =  18.889008591154642\n",
      "loss =  18.88331604956263\n",
      "loss =  18.87767221057584\n",
      "loss =  18.872076506146374\n",
      "loss =  18.866528375608173\n",
      "loss =  18.861027265597084\n",
      "loss =  18.85557262997049\n",
      "loss =  18.850163929726673\n",
      "loss =  18.844800632923967\n",
      "loss =  18.83948221459977\n",
      "loss =  18.834208156689456\n",
      "loss =  18.82897794794535\n",
      "loss =  18.823791083855657\n",
      "loss =  18.8186470665636\n",
      "loss =  18.81354540478666\n",
      "loss =  18.808485613736057\n",
      "loss =  18.803467215036502\n",
      "loss =  18.798489736646225\n",
      "loss =  18.793552712777384\n",
      "loss =  18.788655683816852\n",
      "loss =  18.78379819624742\n",
      "loss =  18.778979802569467\n",
      "loss =  18.774200061223112\n",
      "loss =  18.76945853651088\n",
      "loss =  18.76475479852093\n",
      "loss =  18.760088423050824\n",
      "loss =  18.75545899153192\n",
      "loss =  18.75086609095435\n",
      "loss =  18.746309313792633\n",
      "loss =  18.741788257931955\n",
      "loss =  18.73730252659511\n",
      "loss =  18.732851728270088\n",
      "loss =  18.728435476638406\n",
      "loss =  18.724053390504118\n",
      "loss =  18.719705093723526\n",
      "loss =  18.715390215135645\n",
      "loss =  18.711108388493383\n",
      "loss =  18.70685925239545\n",
      "loss =  18.702642450219052\n",
      "loss =  18.69845763005329\n",
      "loss =  18.69430444463335\n",
      "loss =  18.690182551275417\n",
      "loss =  18.68609161181238\n",
      "loss =  18.682031292530286\n",
      "loss =  18.67800126410557\n",
      "loss =  18.674001201543025\n",
      "loss =  18.670030784114548\n",
      "loss =  18.666089695298677\n",
      "loss =  18.662177622720836\n",
      "loss =  18.658294258094386\n",
      "loss =  18.654439297162412\n",
      "loss =  18.65061243964027\n",
      "loss =  18.646813389158883\n",
      "loss =  18.643041853208803\n",
      "loss =  18.63929754308498\n",
      "loss =  18.635580173832306\n",
      "loss =  18.631889464191886\n",
      "loss =  18.628225136548025\n",
      "loss =  18.624586916875955\n",
      "loss =  18.62097453469028\n",
      "loss =  18.61738772299412\n",
      "loss =  18.613826218228997\n",
      "loss =  18.61028976022538\n",
      "loss =  18.606778092153963\n",
      "loss =  18.6032909604776\n",
      "loss =  18.599828114903964\n",
      "loss =  18.596389308338832\n",
      "loss =  18.592974296840083\n",
      "loss =  18.589582839572333\n",
      "loss =  18.586214698762234\n",
      "loss =  18.582869639654422\n",
      "loss =  18.579547430468107\n",
      "loss =  18.576247842354288\n",
      "loss =  18.5729706493536\n",
      "loss =  18.569715628354786\n",
      "loss =  18.56648255905378\n",
      "loss =  18.563271223913393\n",
      "loss =  18.560081408123565\n",
      "loss =  18.556912899562274\n",
      "loss =  18.553765488756973\n",
      "loss =  18.550638968846606\n",
      "loss =  18.547533135544224\n",
      "loss =  18.544447787100125\n",
      "loss =  18.54138272426557\n",
      "loss =  18.538337750257032\n",
      "loss =  18.53531267072099\n",
      "loss =  18.532307293699255\n",
      "loss =  18.529321429594802\n",
      "loss =  18.526354891138144\n",
      "loss =  18.5234074933542\n",
      "loss =  18.52047905352967\n",
      "loss =  18.517569391180896\n",
      "loss =  18.514678328022224\n",
      "loss =  18.511805687934842\n",
      "loss =  18.508951296936083\n",
      "loss =  18.506114983149203\n",
      "loss =  18.503296576773632\n",
      "loss =  18.500495910055655\n",
      "loss =  18.49771281725954\n",
      "loss =  18.494947134639148\n",
      "loss =  18.492198700409915\n",
      "loss =  18.489467354721302\n",
      "loss =  18.486752939629675\n",
      "loss =  18.484055299071542\n",
      "loss =  18.48137427883728\n",
      "loss =  18.478709726545183\n",
      "loss =  18.47606149161598\n",
      "loss =  18.473429425247673\n",
      "loss =  18.470813380390826\n",
      "loss =  18.46821321172417\n",
      "loss =  18.465628775630634\n",
      "loss =  18.463059930173713\n",
      "loss =  18.460506535074188\n",
      "loss =  18.457968451687236\n",
      "loss =  18.455445542979863\n",
      "loss =  18.452937673508675\n",
      "loss =  18.45044470939803\n",
      "loss =  18.44796651831847\n",
      "loss =  18.445502969465522\n",
      "loss =  18.443053933538792\n",
      "loss =  18.440619282721407\n",
      "loss =  18.438198890659745\n",
      "loss =  18.435792632443498\n",
      "loss =  18.433400384586\n",
      "loss =  18.431022025004914\n",
      "loss =  18.428657433003128\n",
      "loss =  18.42630648925004\n",
      "loss =  18.423969075763043\n",
      "loss =  18.421645075889334\n",
      "loss =  18.419334374287978\n",
      "loss =  18.417036856912286\n",
      "loss =  18.414752410992385\n",
      "loss =  18.412480925018123\n",
      "loss =  18.410222288722185\n",
      "loss =  18.4079763930635\n",
      "loss =  18.405743130210855\n",
      "loss =  18.403522393526803\n",
      "loss =  18.40131407755177\n",
      "loss =  18.39911807798844\n",
      "loss =  18.39693429168634\n",
      "loss =  18.394762616626668\n",
      "loss =  18.392602951907367\n",
      "loss =  18.390455197728404\n",
      "loss =  18.388319255377247\n",
      "loss =  18.386195027214615\n",
      "loss =  18.3840824166604\n",
      "loss =  18.381981328179787\n",
      "loss =  18.37989166726963\n",
      "loss =  18.377813340444977\n",
      "loss =  18.37574625522584\n",
      "loss =  18.373690320124126\n",
      "loss =  18.371645444630776\n",
      "loss =  18.369611539203106\n",
      "loss =  18.36758851525231\n",
      "loss =  18.36557628513117\n",
      "loss =  18.363574762121928\n",
      "loss =  18.361583860424354\n",
      "loss =  18.359603495143972\n",
      "loss =  18.35763358228049\n",
      "loss =  18.355674038716334\n",
      "loss =  18.353724782205447\n",
      "loss =  18.351785731362153\n",
      "loss =  18.34985680565026\n",
      "loss =  18.34793792537226\n",
      "loss =  18.34602901165875\n",
      "loss =  18.344129986457943\n",
      "loss =  18.342240772525386\n",
      "loss =  18.340361293413782\n",
      "loss =  18.338491473462984\n",
      "loss =  18.33663123779015\n",
      "loss =  18.334780512279988\n",
      "loss =  18.332939223575195\n",
      "loss =  18.331107299066996\n",
      "loss =  18.32928466688585\n",
      "loss =  18.327471255892263\n",
      "loss =  18.32566699566775\n",
      "loss =  18.323871816505903\n",
      "loss =  18.32208564940363\n",
      "loss =  18.320308426052463\n",
      "loss =  18.318540078830047\n",
      "loss =  18.316780540791694\n",
      "loss =  18.31502974566211\n",
      "loss =  18.313287627827183\n",
      "loss =  18.31155412232596\n",
      "loss =  18.30982916484265\n",
      "loss =  18.30811269169882\n",
      "loss =  18.306404639845645\n",
      "loss =  18.304704946856297\n",
      "loss =  18.303013550918426\n",
      "loss =  18.301330390826763\n",
      "loss =  18.299655405975795\n",
      "loss =  18.297988536352594\n",
      "loss =  18.296329722529677\n",
      "loss =  18.29467890565804\n",
      "loss =  18.293036027460218\n",
      "loss =  18.291401030223504\n",
      "loss =  18.28977385679322\n",
      "loss =  18.288154450566093\n",
      "loss =  18.28654275548373\n",
      "loss =  18.28493871602617\n",
      "loss =  18.283342277205534\n",
      "loss =  18.281753384559767\n",
      "loss =  18.280171984146453\n",
      "loss =  18.278598022536716\n",
      "loss =  18.277031446809215\n",
      "loss =  18.275472204544215\n",
      "loss =  18.273920243817734\n",
      "loss =  18.272375513195783\n",
      "loss =  18.270837961728663\n",
      "loss =  18.269307538945363\n",
      "loss =  18.267784194848016\n",
      "loss =  18.266267879906437\n",
      "loss =  18.264758545052747\n",
      "loss =  18.263256141676028\n",
      "loss =  18.261760621617114\n",
      "loss =  18.26027193716339\n",
      "loss =  18.2587900410437\n",
      "loss =  18.257314886423302\n",
      "loss =  18.255846426898902\n",
      "loss =  18.254384616493766\n",
      "loss =  18.25292940965285\n",
      "loss =  18.251480761238057\n",
      "loss =  18.250038626523505\n",
      "loss =  18.248602961190898\n",
      "loss =  18.24717372132492\n",
      "loss =  18.245750863408734\n",
      "loss =  18.24433434431948\n",
      "loss =  18.242924121323906\n",
      "loss =  18.24152015207398\n",
      "loss =  18.240122394602643\n",
      "loss =  18.238730807319527\n",
      "loss =  18.2373453490068\n",
      "loss =  18.235965978815035\n",
      "loss =  18.23459265625912\n",
      "loss =  18.23322534121428\n",
      "loss =  18.23186399391205\n",
      "loss =  18.230508574936405\n",
      "loss =  18.229159045219884\n",
      "loss =  18.227815366039756\n",
      "loss =  18.226477499014273\n",
      "loss =  18.225145406098935\n",
      "loss =  18.223819049582836\n",
      "loss =  18.222498392085026\n",
      "loss =  18.221183396550945\n",
      "loss =  18.219874026248874\n",
      "loss =  18.218570244766475\n",
      "loss =  18.217272016007314\n",
      "loss =  18.2159793041875\n",
      "loss =  18.214692073832286\n",
      "loss =  18.21341028977281\n",
      "loss =  18.212133917142772\n",
      "loss =  18.210862921375227\n",
      "loss =  18.209597268199403\n",
      "loss =  18.208336923637532\n",
      "loss =  18.207081854001764\n",
      "loss =  18.205832025891066\n",
      "loss =  18.204587406188214\n",
      "loss =  18.203347962056792\n",
      "loss =  18.20211366093823\n",
      "loss =  18.200884470548893\n",
      "loss =  18.19966035887717\n",
      "loss =  18.19844129418067\n",
      "loss =  18.19722724498337\n",
      "loss =  18.19601818007285\n",
      "loss =  18.194814068497553\n",
      "loss =  18.193614879564066\n",
      "loss =  18.19242058283444\n",
      "loss =  18.191231148123574\n",
      "loss =  18.190046545496553\n",
      "loss =  18.188866745266118\n",
      "loss =  18.18769171799008\n",
      "loss =  18.186521434468826\n",
      "loss =  18.185355865742817\n",
      "loss =  18.18419498309014\n",
      "loss =  18.183038758024075\n",
      "loss =  18.181887162290707\n",
      "loss =  18.18074016786654\n",
      "loss =  18.179597746956166\n",
      "loss =  18.178459871989965\n",
      "loss =  18.177326515621804\n",
      "loss =  18.176197650726778\n",
      "loss =  18.175073250398988\n",
      "loss =  18.173953287949345\n",
      "loss =  18.172837736903375\n",
      "loss =  18.171726570999088\n",
      "loss =  18.170619764184824\n",
      "loss =  18.169517290617183\n",
      "loss =  18.168419124658932\n",
      "loss =  18.16732524087696\n",
      "loss =  18.166235614040243\n",
      "loss =  18.165150219117855\n",
      "loss =  18.16406903127698\n",
      "loss =  18.162992025880946\n",
      "loss =  18.161919178487317\n",
      "loss =  18.160850464845954\n",
      "loss =  18.159785860897152\n",
      "loss =  18.158725342769753\n",
      "loss =  18.157668886779316\n",
      "loss =  18.156616469426282\n",
      "loss =  18.15556806739419\n",
      "loss =  18.154523657547866\n",
      "loss =  18.153483216931697\n",
      "loss =  18.152446722767863\n",
      "loss =  18.15141415245463\n",
      "loss =  18.150385483564655\n",
      "loss =  18.149360693843285\n",
      "loss =  18.148339761206916\n",
      "loss =  18.14732266374134\n",
      "loss =  18.146309379700117\n",
      "loss =  18.14529988750299\n",
      "loss =  18.14429416573427\n",
      "loss =  18.14329219314128\n",
      "loss =  18.142293948632815\n",
      "loss =  18.141299411277593\n",
      "loss =  18.140308560302735\n",
      "loss =  18.13932137509228\n",
      "loss =  18.1383378351857\n",
      "loss =  18.13735792027642\n",
      "loss =  18.136381610210382\n",
      "loss =  18.13540888498462\n",
      "loss =  18.134439724745818\n",
      "loss =  18.133474109788928\n",
      "loss =  18.132512020555783\n",
      "loss =  18.131553437633713\n",
      "loss =  18.130598341754208\n",
      "loss =  18.129646713791573\n",
      "loss =  18.12869853476159\n",
      "loss =  18.127753785820218\n",
      "loss =  18.12681244826231\n",
      "loss =  18.125874503520304\n",
      "loss =  18.124939933162974\n",
      "loss =  18.124008718894178\n",
      "loss =  18.123080842551595\n",
      "loss =  18.12215628610553\n",
      "loss =  18.12123503165768\n",
      "loss =  18.12031706143995\n",
      "loss =  18.119402357813243\n",
      "loss =  18.118490903266302\n",
      "loss =  18.11758268041456\n",
      "loss =  18.116677671998968\n",
      "loss =  18.11577586088487\n",
      "loss =  18.114877230060877\n",
      "loss =  18.113981762637767\n",
      "loss =  18.113089441847364\n",
      "loss =  18.112200251041468\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss =  18.111314173690772\n",
      "loss =  18.110431193383793\n",
      "loss =  18.109551293825827\n",
      "loss =  18.108674458837903\n",
      "loss =  18.107800672355754\n",
      "loss =  18.10692991842879\n",
      "loss =  18.106062181219098\n",
      "loss =  18.10519744500044\n",
      "loss =  18.104335694157257\n",
      "loss =  18.10347691318371\n",
      "loss =  18.102621086682696\n",
      "loss =  18.101768199364898\n",
      "loss =  18.10091823604784\n",
      "loss =  18.10007118165494\n",
      "loss =  18.099227021214592\n",
      "loss =  18.098385739859253\n",
      "loss =  18.097547322824525\n",
      "loss =  18.096711755448254\n",
      "loss =  18.095879023169662\n",
      "loss =  18.095049111528432\n",
      "loss =  18.09422200616387\n",
      "loss =  18.09339769281404\n",
      "loss =  18.092576157314877\n",
      "loss =  18.09175738559939\n",
      "loss =  18.090941363696796\n",
      "loss =  18.09012807773171\n",
      "loss =  18.0893175139233\n",
      "loss =  18.088509658584524\n",
      "loss =  18.087704498121276\n",
      "loss =  18.086902019031637\n",
      "loss =  18.086102207905054\n",
      "loss =  18.08530505142159\n",
      "loss =  18.08451053635114\n",
      "loss =  18.083718649552676\n",
      "loss =  18.08292937797349\n",
      "loss =  18.082142708648444\n",
      "loss =  18.08135862869925\n",
      "loss =  18.080577125333704\n",
      "loss =  18.079798185845004\n",
      "loss =  18.079021797611002\n",
      "loss =  18.078247948093505\n",
      "loss =  18.07747662483758\n",
      "loss =  18.076707815470844\n",
      "loss =  18.075941507702794\n",
      "loss =  18.07517768932411\n",
      "loss =  18.07441634820599\n",
      "loss =  18.073657472299473\n",
      "loss =  18.072901049634787\n",
      "loss =  18.072147068320696\n",
      "loss =  18.071395516543838\n",
      "loss =  18.070646382568103\n",
      "loss =  18.069899654733973\n",
      "loss =  18.06915532145792\n",
      "loss =  18.06841337123176\n",
      "loss =  18.067673792622053\n",
      "loss =  18.066936574269487\n",
      "loss =  18.066201704888257\n",
      "loss =  18.0654691732655\n",
      "loss =  18.064738968260674\n",
      "loss =  18.064011078804974\n",
      "loss =  18.063285493900764\n",
      "loss =  18.062562202620995\n",
      "loss =  18.061841194108624\n",
      "loss =  18.061122457576065\n",
      "loss =  18.06040598230462\n",
      "loss =  18.059691757643932\n",
      "loss =  18.058979773011426\n",
      "loss =  18.058270017891775\n",
      "loss =  18.057562481836364\n",
      "loss =  18.05685715446275\n",
      "loss =  18.056154025454127\n",
      "loss =  18.055453084558827\n",
      "loss =  18.054754321589787\n",
      "loss =  18.054057726424023\n",
      "loss =  18.05336328900215\n",
      "loss =  18.052670999327862\n",
      "loss =  18.05198084746744\n",
      "loss =  18.051292823549247\n",
      "loss =  18.050606917763254\n",
      "loss =  18.049923120360557\n",
      "loss =  18.04924142165287\n",
      "loss =  18.048561812012085\n",
      "loss =  18.04788428186978\n",
      "loss =  18.047208821716765\n",
      "loss =  18.046535422102604\n",
      "loss =  18.045864073635173\n",
      "loss =  18.0451947669802\n",
      "loss =  18.044527492860816\n",
      "loss =  18.04386224205711\n",
      "loss =  18.04319900540569\n",
      "loss =  18.042537773799243\n",
      "loss =  18.04187853818611\n",
      "loss =  18.041221289569847\n",
      "loss =  18.040566019008804\n",
      "loss =  18.039912717615714\n",
      "loss =  18.039261376557256\n",
      "loss =  18.038611987053667\n",
      "loss =  18.037964540378308\n",
      "loss =  18.03731902785728\n",
      "loss =  18.036675440869004\n",
      "loss =  18.036033770843833\n",
      "loss =  18.035394009263662\n",
      "loss =  18.034756147661525\n",
      "loss =  18.034120177621222\n",
      "loss =  18.03348609077692\n",
      "loss =  18.032853878812773\n",
      "loss =  18.032223533462567\n",
      "loss =  18.031595046509327\n",
      "loss =  18.03096840978495\n",
      "loss =  18.030343615169837\n",
      "loss =  18.02972065459254\n",
      "loss =  18.02909952002939\n",
      "loss =  18.028480203504152\n",
      "loss =  18.02786269708765\n",
      "loss =  18.027246992897442\n",
      "loss =  18.02663308309746\n",
      "loss =  18.026020959897664\n",
      "loss =  18.02541061555371\n",
      "loss =  18.0248020423666\n",
      "loss =  18.024195232682356\n",
      "loss =  18.02359017889168\n",
      "loss =  18.022986873429637\n",
      "loss =  18.02238530877532\n",
      "loss =  18.02178547745152\n",
      "loss =  18.02118737202443\n",
      "loss =  18.020590985103286\n",
      "loss =  18.019996309340097\n",
      "loss =  18.019403337429313\n",
      "loss =  18.0188120621075\n",
      "loss =  18.018222476153063\n",
      "loss =  18.017634572385923\n",
      "loss =  18.017048343667213\n",
      "loss =  18.016463782899002\n",
      "loss =  18.01588088302397\n",
      "loss =  18.015299637025137\n",
      "loss =  18.014720037925564\n",
      "loss =  18.014142078788048\n",
      "loss =  18.013565752714875\n",
      "loss =  18.012991052847493\n",
      "loss =  18.012417972366265\n",
      "loss =  18.01184650449017\n",
      "loss =  18.01127664247654\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let us train the neural network to learn y = sin(x)\n",
    "\n",
    "nn = NeuralNetwork([1, 100, 1],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "# generate a sequence of randon values of x and y=sin(x)\n",
    "# as the training set. Generate 10000 input/output pairs\n",
    "# We need to reshape to get the matrix in a linear form\n",
    "# because there is one x = [x1, x2, .......  x1000]\n",
    "\n",
    "X = 2*np.pi*np.random.rand(1000).reshape(1, -1)\n",
    "\n",
    "# y is simply the sine of x and y = [sin(x1), sin(x2), ..... sin(x1000)]\n",
    "y = np.sin(X)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "\n",
    "# train for 1000 epochs\n",
    "nn.train(X, y, epochs=1000, batch_size=64, lr = .1)\n",
    "\n",
    "# calculate the prediction sin(x) for the input X\n",
    "# we only need the a's because this is the output\n",
    "# we do not need the z's so we use that as _\n",
    "\n",
    "_, a_s = nn.feedforward(X)\n",
    "\n",
    "# Let us see how well the nn worked\n",
    "# Plot y and predicted value to see if they \n",
    "# match.\n",
    "plt.scatter(X.flatten(), y.flatten())\n",
    "plt.scatter(X.flatten(), a_s[-1].flatten())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
