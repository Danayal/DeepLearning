{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/izualkernan/opt/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Precision/Recall and ROC Curves \n",
    "\n",
    "COE49412 \n",
    "\n",
    "Imran Zualkernan\n",
    "\n",
    "Spring 2020\n",
    "\n",
    "In this notebook we explore how Precision/Recall and ROC Curves \n",
    "are actually derived from outputs of a neural network.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing backpropagation\n",
    "## Source: \n",
    "## https://medium.com/@a.mirzaei69/implement-a-neural-network-from-scratch-with-python-numpy-backpropagation-e82b70caa9bb\n",
    "## comments added to the original code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    \n",
    "    # The constructor takes \n",
    "    # 1. layers representing the number of nodes\n",
    "    # 2. activations representing the activation functions to choose in\n",
    "    #    each layer. \n",
    "    \n",
    "    def __init__(self, layers = [2 , 10, 1], activations=['sigmoid', 'sigmoid']):\n",
    "        \n",
    "        # check to make sure that no. of layers is one more than \n",
    "        # no. of activation functions because the input layer \n",
    "        # has no activation function.\n",
    "        \n",
    "        assert(len(layers) == len(activations)+1)\n",
    "        \n",
    "        # define the local variables layers and activations\n",
    "        self.layers = layers\n",
    "        self.activations = activations\n",
    "        \n",
    "        # initialize weights and biases as two lists to hold\n",
    "        # weights and biases for each layer\n",
    "        \n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # create random weights for biases and weights\n",
    "        # for each of the layes.\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            self.weights.append(np.random.randn(layers[i+1], layers[i]))\n",
    "            self.biases.append(np.random.randn(layers[i+1], 1))\n",
    "    \n",
    "    # do feedforward for x\n",
    "    # where x is an input\n",
    "    \n",
    "    # return a list of a's and z's as expected.\n",
    "    # a's and z's are layer by layer\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        # make a copy of x\n",
    "        a = np.copy(x)\n",
    "        \n",
    "        # this variable will contain all the z's\n",
    "        z_s = []\n",
    "        \n",
    "        # this variable will contain all the a's\n",
    "        # the output of the input layer is simply x which is the\n",
    "        # input. So we initialize the a_s to contain a. \n",
    "        a_s = [a]\n",
    "        \n",
    "        # for each layer do\n",
    "        for i in range(len(self.weights)):\n",
    "            \n",
    "            # retrieve the appropriate activation function\n",
    "            activation_function = self.getActivationFunction(self.activations[i])\n",
    "            \n",
    "            # create z_s by z = w.a + b for each layer\n",
    "            z_s.append(self.weights[i].dot(a) + self.biases[i])\n",
    "            \n",
    "            # create z_a by a = f(row) -- \n",
    "            # note that we apply it to the last element only \n",
    "            # by using the -1 notation. We only want to apply\n",
    "            # the activation function to the last layer just \n",
    "            # added.\n",
    "            \n",
    "            a = activation_function(z_s[-1])\n",
    "            \n",
    "            # keep track of the new activation or a_s \n",
    "            a_s.append(a)\n",
    "            \n",
    "            # return both z_s and a_s \n",
    "            # we will have z_s and a_s for each layer \n",
    "        return (z_s, a_s)\n",
    "\n",
    "    \n",
    "    # takes the y -- the actual answer, a's and z's and \n",
    "    # calculates the dLoss/dw and dLoss/db\n",
    "    \n",
    "    def backpropagation(self,y, z_s, a_s):\n",
    "        \n",
    "        # initialize list of dLoss/dw and dLoss/db\n",
    "        dw = []  # dLoss/dw\n",
    "        db = []  # dLoss/db\n",
    "        \n",
    "        # create an empty list of deltas, one for each weight\n",
    "        deltas = [None] * len(self.weights)  # delta = dLoss/dz  known as error for each layer\n",
    "        \n",
    "        \n",
    "        # start from the back and insert the last layer error\n",
    "        # based on the square loss function. Note -1 is used to \n",
    "        # fill things from the back of the list \n",
    "        # also note that we need to use the derivative function \n",
    "        # for the activation function.\n",
    "        # note that we do not need to use the 2 in the loss function derivation\n",
    "        \n",
    "        # again note this is for the last layer only!\n",
    "        \n",
    "        deltas[-1] = ((y-a_s[-1])*(self.getDerivitiveActivationFunction(self.activations[-1]))(z_s[-1]))\n",
    "        \n",
    "        \n",
    "        # Perform BackPropagation\n",
    "        \n",
    "        # for the rest of the deltas, go in reverse order\n",
    "        for i in reversed(range(len(deltas)-1)):\n",
    "            deltas[i] = self.weights[i+1].T.dot(deltas[i+1])*(self.getDerivitiveActivationFunction(self.activations[i])(z_s[i]))        \n",
    "        \n",
    "        # now we need to update the weights based on the calculated\n",
    "        # deltas\n",
    "        \n",
    "        #now we will determine the batch size from the first dimension \n",
    "        #of shape of y. We simply want to see how many test cases are there\n",
    "        #for example there may be 10 y's; one for each x. \n",
    "        \n",
    "        batch_size = y.shape[1]\n",
    "        \n",
    "        # determine the two derivatives by taking \n",
    "        # the average according to batch sizes \n",
    "    \n",
    "        \n",
    "        db = [d.dot(np.ones((batch_size,1)))/float(batch_size) for d in deltas]\n",
    "        dw = [d.dot(a_s[i].T)/float(batch_size) for i,d in enumerate(deltas)]\n",
    "        \n",
    "        # return the derivitives respect to weight matrix and biases\n",
    "        return dw, db\n",
    "\n",
    "    \n",
    "    # Now we will write the main training function that uses\n",
    "    # feedforward and backpropagation many times (called epochs)\n",
    "    # lr (learning rate) is the eta in our equations.\n",
    "    \n",
    "    def train(self, x, y, batch_size=10, epochs=100, lr = 0.01):\n",
    "    \n",
    "    # update weights and biases based on the output\n",
    "    # for the number of epochs\n",
    "    \n",
    "        for e in range(epochs): \n",
    "            i=0\n",
    "            \n",
    "            # Do the training in batches\n",
    "            # each batch is a subset of the original \n",
    "            # data \n",
    "            \n",
    "            while(i<len(y)):\n",
    "                \n",
    "                # extract a batch\n",
    "                x_batch = x[i:i+batch_size]\n",
    "                y_batch = y[i:i+batch_size]\n",
    "                \n",
    "                # update i for the next batches\n",
    "                i = i+batch_size\n",
    "                \n",
    "                # do the feedforward for the batch and update the weights\n",
    "                # based on the average loss for each weight for the whole\n",
    "                # batch.\n",
    "                \n",
    "                z_s, a_s = self.feedforward(x_batch)\n",
    "                \n",
    "                # do the back propagation \n",
    "                dw, db = self.backpropagation(y_batch, z_s, a_s)\n",
    "                \n",
    "                \n",
    "                # update the weights for each pair of weights and dw\n",
    "                # and biases and db\n",
    "                \n",
    "                self.weights = [w+lr*dweight for w,dweight in  zip(self.weights, dw)]\n",
    "                self.biases = [w+lr*dbias for w,dbias in  zip(self.biases, db)]\n",
    "                \n",
    "                # print the loss using a built in function \n",
    "                # to calculate the loss\n",
    "                \n",
    "                #print(\"loss = \", np.linalg.norm(a_s[-1]-y_batch) )\n",
    "    \n",
    "    \n",
    "    # This function is being used to return an activation function \n",
    "    # depending on its weights\n",
    "    \n",
    "    @staticmethod\n",
    "    def getActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            return lambda x : np.exp(x)/(1+np.exp(x))\n",
    "        elif(name == 'linear'):\n",
    "            return lambda x : x\n",
    "        elif(name == 'relu'):\n",
    "            def relu(x):\n",
    "                y = np.copy(x)\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: x\n",
    "    \n",
    "    # This function returns the derivative of a function depending\n",
    "    # on its name.\n",
    "    \n",
    "    @staticmethod\n",
    "    def getDerivitiveActivationFunction(name):\n",
    "        if(name == 'sigmoid'):\n",
    "            sig = lambda x : np.exp(x)/(1+np.exp(x))\n",
    "            return lambda x :sig(x)*(1-sig(x)) \n",
    "        elif(name == 'linear'):\n",
    "            return lambda x: 1\n",
    "        elif(name == 'relu'):\n",
    "            def relu_diff(x):\n",
    "                y = np.copy(x)\n",
    "                y[y>=0] = 1\n",
    "                y[y<0] = 0\n",
    "                return y\n",
    "            return relu_diff\n",
    "        else:\n",
    "            print('Unknown activation function. linear is used')\n",
    "            return lambda x: 1     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The purpose of this exercise is to try to model a \n",
    "# step function using the neural network called xy().\n",
    "\n",
    "# Since the output of neural network is using sigmoid\n",
    "# we will get a number between 0 and 1. The threshold\n",
    "# function threshold(x,p) is used to force a value\n",
    "# to either 0 or 1.\n",
    "\n",
    "\n",
    "# use to define a step function - array function\n",
    "# we want to use the neural network to model this \n",
    "# function\n",
    "\n",
    "def xy(x):\n",
    "    y = np.copy(x)\n",
    "    y[y>=0.5] = 1\n",
    "    y[y<0.5] = 0\n",
    "    return y\n",
    "\n",
    "# used to define a threshold -- array function\n",
    "# threshold is p, if less than p y is 0\n",
    "# otherwise y is 1\n",
    "def threshold(x,p):\n",
    "    y = np.copy(x)\n",
    "    y[y<=p] = 0\n",
    "    y[y>p] = 1\n",
    "    return y\n",
    "\n",
    "# generate X and y\n",
    "X = np.random.rand(1000).reshape(1, -1)\n",
    "y = xy(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9364376 , 0.03050578, 0.02511642, 0.92468907, 0.82369299,\n",
       "        0.58378764, 0.47552957, 0.04274506, 0.06073448, 0.11091192,\n",
       "        0.28620557, 0.14687851, 0.37779856, 0.9286577 , 0.68778378,\n",
       "        0.79642411, 0.8054318 , 0.6187895 , 0.07147288, 0.20200992,\n",
       "        0.93845881, 0.87321724, 0.04845095, 0.94960115, 0.54781262,\n",
       "        0.81138662, 0.31879064, 0.93289563, 0.16469   , 0.34108459,\n",
       "        0.30124278, 0.17561451, 0.3266611 , 0.52896783, 0.48469268,\n",
       "        0.02374083, 0.06368363, 0.95039195, 0.74214672, 0.92346187,\n",
       "        0.12341951, 0.96382043, 0.43540561, 0.64169435, 0.94776366,\n",
       "        0.88112708, 0.63105448, 0.79197782, 0.03663611, 0.03856711,\n",
       "        0.75594485, 0.82301436, 0.27916874, 0.08824218, 0.95144949,\n",
       "        0.95989979, 0.94632925, 0.76618917, 0.1663844 , 0.84468264,\n",
       "        0.90892518, 0.69284109, 0.83036715, 0.47694032, 0.67364436,\n",
       "        0.84711042, 0.76886536, 0.08712348, 0.94629817, 0.05444137,\n",
       "        0.60713291, 0.15792744, 0.20383663, 0.95574382, 0.03881778,\n",
       "        0.11528698, 0.35247302, 0.77179273, 0.95651197, 0.26065179,\n",
       "        0.07544075, 0.02798821, 0.58874184, 0.12500653, 0.21273226,\n",
       "        0.52606444, 0.60071077, 0.92543023, 0.0460929 , 0.15841851,\n",
       "        0.05216702, 0.03230818, 0.18691134, 0.33812816, 0.0730678 ,\n",
       "        0.43951916, 0.20450821, 0.89059983, 0.02553006, 0.07697706,\n",
       "        0.03739947, 0.17646394, 0.63032771, 0.19961741, 0.65969584,\n",
       "        0.13851188, 0.03824009, 0.2723191 , 0.93490755, 0.03110945,\n",
       "        0.05557066, 0.34889365, 0.11397899, 0.67840459, 0.09978728,\n",
       "        0.5290749 , 0.15228897, 0.15535408, 0.5552409 , 0.20773352,\n",
       "        0.41331722, 0.14334835, 0.06484114, 0.85176973, 0.81709093,\n",
       "        0.76837534, 0.82524603, 0.69654686, 0.38927449, 0.67181444,\n",
       "        0.12972637, 0.88711718, 0.04446347, 0.88752822, 0.41073726,\n",
       "        0.4240402 , 0.06912966, 0.96287868, 0.12204173, 0.33988409,\n",
       "        0.41979081, 0.47611952, 0.80006648, 0.06824882, 0.93010894,\n",
       "        0.339048  , 0.67827073, 0.16420887, 0.22197688, 0.06161083,\n",
       "        0.80520419, 0.92578349, 0.58803733, 0.03551879, 0.10294186,\n",
       "        0.19964606, 0.91709165, 0.85745166, 0.74630962, 0.14905193,\n",
       "        0.35795317, 0.09459357, 0.72204086, 0.69232323, 0.60266983,\n",
       "        0.18916132, 0.24290251, 0.70981371, 0.02354094, 0.11396607,\n",
       "        0.32599501, 0.9560772 , 0.83187944, 0.03583167, 0.19578656,\n",
       "        0.62436088, 0.16295462, 0.05298667, 0.95321183, 0.92309039,\n",
       "        0.0439021 , 0.95423741, 0.53540209, 0.94455893, 0.82895395,\n",
       "        0.96272209, 0.57765546, 0.8824031 , 0.90304628, 0.87541715,\n",
       "        0.83565332, 0.02733626, 0.6946893 , 0.9125709 , 0.05840437,\n",
       "        0.93809564, 0.91525278, 0.23220993, 0.02448602, 0.02858135,\n",
       "        0.44506342, 0.89497836, 0.06777273, 0.31952739, 0.95359639,\n",
       "        0.15658014, 0.60560982, 0.81132353, 0.72407788, 0.92987028,\n",
       "        0.95391703, 0.10570997, 0.96094254, 0.8168765 , 0.13823461,\n",
       "        0.45073006, 0.50583735, 0.20346697, 0.279513  , 0.8930716 ,\n",
       "        0.94272853, 0.06438824, 0.10159901, 0.94713746, 0.95844945,\n",
       "        0.61679519, 0.94915587, 0.02360543, 0.37609778, 0.11167245,\n",
       "        0.73200679, 0.07722872, 0.95439164, 0.0762483 , 0.34532121,\n",
       "        0.11120196, 0.3156598 , 0.91876561, 0.76459963, 0.79502617,\n",
       "        0.8492151 , 0.60658103, 0.03247361, 0.0409056 , 0.91009665,\n",
       "        0.29429274, 0.92286619, 0.9186791 , 0.92461899, 0.81213592,\n",
       "        0.93660431, 0.23428218, 0.92952559, 0.80256226, 0.23397659,\n",
       "        0.07187479, 0.84689797, 0.50623159, 0.87656675, 0.03351216,\n",
       "        0.95808192, 0.26076212, 0.10003213, 0.27727375, 0.05586161,\n",
       "        0.16655726, 0.05375951, 0.09751402, 0.64657048, 0.79550864,\n",
       "        0.77324755, 0.89141164, 0.85775923, 0.8205943 , 0.79857326,\n",
       "        0.4240743 , 0.36217018, 0.06588941, 0.95385965, 0.65139625,\n",
       "        0.04987495, 0.87563021, 0.03272673, 0.73360502, 0.65032402,\n",
       "        0.90056697, 0.87639347, 0.51569753, 0.44023406, 0.9091033 ,\n",
       "        0.67659418, 0.13697904, 0.82594253, 0.90854536, 0.6636753 ,\n",
       "        0.66921381, 0.06680516, 0.04276861, 0.06887635, 0.4551414 ,\n",
       "        0.89348863, 0.29001951, 0.0270229 , 0.86966668, 0.95431469,\n",
       "        0.94295216, 0.51355834, 0.73866222, 0.13713393, 0.21586221,\n",
       "        0.03072801, 0.36227047, 0.34730701, 0.54482356, 0.03002239,\n",
       "        0.87068815, 0.87538424, 0.23261705, 0.3953972 , 0.03258963,\n",
       "        0.08770328, 0.73180965, 0.19947561, 0.0272378 , 0.88993204,\n",
       "        0.81487778, 0.90171811, 0.08254819, 0.08114498, 0.55642636,\n",
       "        0.02463719, 0.56648026, 0.11553774, 0.93172233, 0.07183963,\n",
       "        0.33444653, 0.71567166, 0.15055605, 0.50879   , 0.03817466,\n",
       "        0.76568142, 0.39201154, 0.79994799, 0.54697652, 0.56746912,\n",
       "        0.96326111, 0.88265242, 0.63581249, 0.39209505, 0.80605997,\n",
       "        0.02516563, 0.23494594, 0.05799317, 0.03455484, 0.06719119,\n",
       "        0.85502171, 0.90132308, 0.92886153, 0.96384519, 0.60385432,\n",
       "        0.38729788, 0.25039783, 0.5523455 , 0.15207444, 0.73318032,\n",
       "        0.90095597, 0.95445717, 0.55150307, 0.82846423, 0.68977546,\n",
       "        0.09264829, 0.87837711, 0.04587211, 0.17278787, 0.95469745,\n",
       "        0.37031847, 0.89497528, 0.39770285, 0.82201182, 0.78622375,\n",
       "        0.08053028, 0.61280985, 0.03368555, 0.28764947, 0.9446781 ,\n",
       "        0.62742522, 0.84658068, 0.9015353 , 0.45210752, 0.96513283,\n",
       "        0.94191715, 0.43111357, 0.37237498, 0.37577298, 0.26841865,\n",
       "        0.96267305, 0.31920916, 0.92900725, 0.53446998, 0.9497517 ,\n",
       "        0.29194014, 0.88016536, 0.05610307, 0.17460725, 0.87024978,\n",
       "        0.54860281, 0.58395548, 0.02528761, 0.03430465, 0.79194717,\n",
       "        0.9341602 , 0.38722421, 0.31832888, 0.75975269, 0.80370807,\n",
       "        0.02785153, 0.95700466, 0.47412603, 0.12082189, 0.92538092,\n",
       "        0.94786814, 0.02437515, 0.89407411, 0.2119895 , 0.15241677,\n",
       "        0.94573675, 0.09662644, 0.34801664, 0.1194257 , 0.87622685,\n",
       "        0.71582933, 0.30105328, 0.88933476, 0.95243313, 0.56038598,\n",
       "        0.92102831, 0.03647987, 0.86633358, 0.08559364, 0.22173927,\n",
       "        0.69999521, 0.10700183, 0.70582192, 0.95963249, 0.37295755,\n",
       "        0.89709437, 0.05343487, 0.04181419, 0.96188475, 0.95923175,\n",
       "        0.48758913, 0.79378236, 0.46776052, 0.74149321, 0.46187419,\n",
       "        0.59197966, 0.58376685, 0.053     , 0.11059083, 0.06785347,\n",
       "        0.83928802, 0.03515443, 0.5187954 , 0.04856606, 0.27214192,\n",
       "        0.53197469, 0.78756019, 0.13084242, 0.68932749, 0.83720631,\n",
       "        0.09578603, 0.62002732, 0.78765694, 0.0292972 , 0.73034466,\n",
       "        0.64708304, 0.36066262, 0.8117816 , 0.34115065, 0.93524505,\n",
       "        0.04963811, 0.79994907, 0.03573894, 0.16256185, 0.02535374,\n",
       "        0.92300011, 0.1163725 , 0.03139553, 0.1011007 , 0.37923226,\n",
       "        0.46403112, 0.18760352, 0.03123617, 0.77953453, 0.11223613,\n",
       "        0.61249946, 0.21606064, 0.93232373, 0.83679668, 0.711716  ,\n",
       "        0.95967568, 0.02558989, 0.6825371 , 0.94151336, 0.73239626,\n",
       "        0.63706178, 0.36735891, 0.32542813, 0.0610581 , 0.92639331,\n",
       "        0.95257844, 0.8843847 , 0.1130143 , 0.81695647, 0.94293867,\n",
       "        0.65792183, 0.32708227, 0.783788  , 0.44917862, 0.3972853 ,\n",
       "        0.44488228, 0.0275099 , 0.82831196, 0.94038596, 0.89793699,\n",
       "        0.07899123, 0.05116759, 0.89486305, 0.08226964, 0.80989374,\n",
       "        0.02564837, 0.35619237, 0.87309171, 0.5327353 , 0.9568333 ,\n",
       "        0.7112876 , 0.95150356, 0.65621364, 0.85496562, 0.11629052,\n",
       "        0.9418149 , 0.75786722, 0.04466438, 0.39395772, 0.66547733,\n",
       "        0.35702127, 0.96396444, 0.19329074, 0.92891816, 0.093727  ,\n",
       "        0.08014058, 0.19699425, 0.09768415, 0.95239475, 0.86823615,\n",
       "        0.33335066, 0.09222667, 0.03283859, 0.30844402, 0.24100261,\n",
       "        0.70642147, 0.95072186, 0.3333938 , 0.11835978, 0.25921812,\n",
       "        0.82564468, 0.74009357, 0.09237647, 0.6663226 , 0.04665268,\n",
       "        0.93213173, 0.69483923, 0.09840486, 0.96054204, 0.02987781,\n",
       "        0.90328661, 0.95064422, 0.40025385, 0.89562489, 0.92030054,\n",
       "        0.25180999, 0.95090664, 0.0528043 , 0.96466663, 0.42715434,\n",
       "        0.02494874, 0.87924238, 0.71314663, 0.29603104, 0.04589256,\n",
       "        0.76903882, 0.42248499, 0.52685555, 0.06688073, 0.6397719 ,\n",
       "        0.55587571, 0.13351362, 0.79363373, 0.61096701, 0.43281868,\n",
       "        0.07817115, 0.91358566, 0.02533497, 0.13833701, 0.83839914,\n",
       "        0.08142727, 0.31224988, 0.93750506, 0.13352203, 0.02830975,\n",
       "        0.79192965, 0.84057592, 0.22861737, 0.83079904, 0.71940918,\n",
       "        0.09481344, 0.92814558, 0.04877214, 0.46321224, 0.49483569,\n",
       "        0.17473493, 0.10614916, 0.75384752, 0.0450047 , 0.10299751,\n",
       "        0.07689443, 0.21993101, 0.5756412 , 0.03044009, 0.04323835,\n",
       "        0.96499692, 0.91137205, 0.92398206, 0.65019598, 0.32012359,\n",
       "        0.93663857, 0.80348645, 0.94999792, 0.80630776, 0.68710972,\n",
       "        0.13665384, 0.95793917, 0.05518269, 0.15705777, 0.15371081,\n",
       "        0.47940035, 0.70290445, 0.80897065, 0.72679634, 0.69027676,\n",
       "        0.35489661, 0.03402867, 0.2052154 , 0.30637954, 0.4779976 ,\n",
       "        0.95372274, 0.03357498, 0.9454144 , 0.02521537, 0.1480108 ,\n",
       "        0.03561294, 0.0419018 , 0.67075101, 0.28547125, 0.7444676 ,\n",
       "        0.71414934, 0.02555053, 0.83931885, 0.91321934, 0.72500371,\n",
       "        0.43016544, 0.23055098, 0.31546989, 0.45383581, 0.42673888,\n",
       "        0.73867178, 0.4919218 , 0.77905844, 0.09317311, 0.30201638,\n",
       "        0.07785529, 0.95317012, 0.0640907 , 0.22768966, 0.1236058 ,\n",
       "        0.51393473, 0.93414373, 0.19003062, 0.80929813, 0.3619181 ,\n",
       "        0.77245027, 0.8929091 , 0.92740751, 0.82536516, 0.03166931,\n",
       "        0.96122599, 0.96383785, 0.89088111, 0.92534547, 0.15932753,\n",
       "        0.03383407, 0.34408714, 0.49484358, 0.17359759, 0.50344439,\n",
       "        0.48986204, 0.17576626, 0.95203511, 0.10927505, 0.38097536,\n",
       "        0.69506905, 0.95798765, 0.47429025, 0.05739517, 0.215294  ,\n",
       "        0.58041367, 0.15125663, 0.81407159, 0.53798652, 0.95420065,\n",
       "        0.17837003, 0.03674314, 0.06664817, 0.9618057 , 0.74369806,\n",
       "        0.03084861, 0.30802691, 0.5761185 , 0.81992603, 0.06670823,\n",
       "        0.15290379, 0.92344405, 0.07168796, 0.8125283 , 0.91019665,\n",
       "        0.43149063, 0.11191612, 0.06123068, 0.3057144 , 0.9432397 ,\n",
       "        0.10296799, 0.11564592, 0.94713416, 0.63319604, 0.81699278,\n",
       "        0.78338654, 0.06937676, 0.50851432, 0.03203883, 0.57315191,\n",
       "        0.93313651, 0.96382945, 0.91861549, 0.68583313, 0.56437642,\n",
       "        0.2964139 , 0.03543209, 0.93708655, 0.87439716, 0.90437364,\n",
       "        0.70220789, 0.81510291, 0.29095861, 0.12270119, 0.88271949,\n",
       "        0.11220052, 0.92097342, 0.93915607, 0.77236967, 0.78915857,\n",
       "        0.02784732, 0.23870598, 0.7210783 , 0.05682579, 0.52971597,\n",
       "        0.0541845 , 0.37609744, 0.83622834, 0.96476367, 0.02856743,\n",
       "        0.84951266, 0.07708126, 0.55981557, 0.74340319, 0.15692842,\n",
       "        0.03943766, 0.8711694 , 0.02596621, 0.20703614, 0.95854168,\n",
       "        0.74196795, 0.07870016, 0.04496948, 0.1089252 , 0.95314835,\n",
       "        0.7529215 , 0.03145618, 0.10899961, 0.63033487, 0.62948868,\n",
       "        0.08576934, 0.23687794, 0.58157033, 0.04789432, 0.03517281,\n",
       "        0.04329942, 0.64483643, 0.73063061, 0.51726035, 0.42129101,\n",
       "        0.04221378, 0.14744549, 0.32563682, 0.02445066, 0.06380482,\n",
       "        0.03176781, 0.86121337, 0.65992885, 0.43513809, 0.30898444,\n",
       "        0.66067128, 0.09708693, 0.05664202, 0.42997481, 0.37163914,\n",
       "        0.10603311, 0.03774868, 0.07072924, 0.66690174, 0.11957928,\n",
       "        0.19227726, 0.18470989, 0.39839316, 0.88444487, 0.83911592,\n",
       "        0.74870508, 0.96052638, 0.27753107, 0.96015005, 0.15541395,\n",
       "        0.0338024 , 0.53776067, 0.88969622, 0.03333023, 0.82346529,\n",
       "        0.08687588, 0.04620124, 0.95759597, 0.94338124, 0.76488233,\n",
       "        0.29061638, 0.82037535, 0.31096029, 0.06050575, 0.79710406,\n",
       "        0.23296164, 0.9341876 , 0.74606275, 0.3264486 , 0.23496571,\n",
       "        0.73284391, 0.39018759, 0.43096426, 0.05592343, 0.86324991,\n",
       "        0.55616184, 0.79368873, 0.08445438, 0.71639288, 0.74264421,\n",
       "        0.08670203, 0.29326865, 0.93105005, 0.04827619, 0.16155049,\n",
       "        0.52134751, 0.11235256, 0.12109737, 0.78050987, 0.13765929,\n",
       "        0.02845778, 0.96077599, 0.49656829, 0.04236151, 0.43011433,\n",
       "        0.93152708, 0.90689137, 0.05602664, 0.19006359, 0.8090223 ,\n",
       "        0.88642199, 0.9622052 , 0.03605902, 0.4086587 , 0.91208965,\n",
       "        0.30485547, 0.75907741, 0.95438328, 0.8830623 , 0.78846742,\n",
       "        0.33644082, 0.75626531, 0.40987322, 0.13104522, 0.03091798,\n",
       "        0.85068171, 0.25332447, 0.75786843, 0.0932474 , 0.03766814,\n",
       "        0.75988183, 0.78915359, 0.03019214, 0.38352844, 0.04368161,\n",
       "        0.18771815, 0.10153739, 0.12748339, 0.48820006, 0.94269239,\n",
       "        0.91331478, 0.57838478, 0.74699979, 0.25088162, 0.13227861,\n",
       "        0.12792796, 0.10579201, 0.9199059 , 0.03000577, 0.90455755,\n",
       "        0.5185909 , 0.95914552, 0.16436127, 0.92981675, 0.27058347,\n",
       "        0.74706831, 0.07242777, 0.05469806, 0.94871488, 0.80502804,\n",
       "        0.1999923 , 0.95879665, 0.03754239, 0.57696784, 0.75046558,\n",
       "        0.58489262, 0.41978238, 0.65481756, 0.02361997, 0.73517528,\n",
       "        0.21205409, 0.78187604, 0.02370228, 0.10509995, 0.05054936,\n",
       "        0.13980067, 0.94092778, 0.12520478, 0.497423  , 0.05567745,\n",
       "        0.9382352 , 0.95654974, 0.36460967, 0.93325201, 0.17063882,\n",
       "        0.02720028, 0.58966898, 0.18246853, 0.70096607, 0.91586201,\n",
       "        0.19860185, 0.52945394, 0.02397759, 0.09037251, 0.65573065,\n",
       "        0.85284674, 0.04181379, 0.17103213, 0.22275198, 0.16630804,\n",
       "        0.11794225, 0.36176368, 0.02355459, 0.91677475, 0.20542689,\n",
       "        0.88634083, 0.91557277, 0.88466363, 0.89630035, 0.10985176,\n",
       "        0.44814427, 0.81436734, 0.84994152, 0.37152823, 0.9479754 ,\n",
       "        0.70997967, 0.09898937, 0.04998001, 0.96449594, 0.94400684]])"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn = NeuralNetwork([1, 100, 1],activations=['sigmoid', 'sigmoid'])\n",
    "\n",
    "# run_once will return the predicted values after training \n",
    "# NN on input (X) and expected output (y)\n",
    "\n",
    "def run_once(X,y,p):\n",
    "    # train for 1000 epochs\n",
    "    nn.train(X, y, epochs=1000, batch_size=64, lr = .1)\n",
    "\n",
    "    # calculate the prediction y for the input X\n",
    "    _, a_s = nn.feedforward(X)\n",
    "\n",
    "    ## transform the output to use the threshold\n",
    "    ## p in this case. This will make the output \n",
    "    ## 0 or 1\n",
    "    \n",
    "    #a_s[-1] = np.apply_along_axis(threshold,0,a_s[-1],p)\n",
    "    \n",
    "    return(a_s[-1])\n",
    "\n",
    "run_once(X,y,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## plot the results of training of a NN \n",
    "# using y and predicted y (py)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Let us see how well the nn worked\n",
    "# Plot y and predicted value to see if they \n",
    "# match.\n",
    "\n",
    "def plot_actual_pred(X,y,py):\n",
    "    plt.figure(1)                # the first figure\n",
    "    plt.subplot(211)             # the first subplot in the first figure\n",
    "    plt.scatter(X.flatten(), y.flatten(), c='g',label='actual')\n",
    "    plt.xlabel(\"actual\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(212)             # the second subplot in the first figure\n",
    "    plt.scatter(X.flatten(), py.flatten(),c = 'r', label='predicted')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.xlabel(\"y\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "plot_actual_pred(X,y,run_once(X,y,0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-181-5843eeaaee36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Now we determine the confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[0;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multiclass\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s is not supported\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# calculate various metrics\n",
    "# We have actual and predicted now.\n",
    "actual = np.copy(y.flatten())\n",
    "predicted = run_once(X,y,0.6).flatten()\n",
    "\n",
    "# Now we determine the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm=confusion_matrix(actual,predicted)\n",
    "print(cm)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(actual, predicted))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(f1_score(actual, predicted))\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "print(precision_score(actual, predicted))\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "print(recall_score(actual, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.8968692449355433, 1.0)\n",
      "(0.9365384615384615, 1.0)\n",
      "(0.9567779960707269, 1.0)\n",
      "(0.9779116465863453, 1.0)\n",
      "(0.9959100204498977, 1.0)\n"
     ]
    }
   ],
   "source": [
    "# grab the precision and recall only from running once\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def precision_recall(X,y,p):\n",
    "    pr = run_once(X,y,p)\n",
    "    actual = np.copy(y.flatten())\n",
    "    predicted = np.copy(pr.flatten())\n",
    "    return precision_score(actual, predicted), recall_score(actual,predicted)\n",
    "\n",
    "precision = np.zeros(10)\n",
    "recall = np.zeros(10)\n",
    "\n",
    "print(precision_recall(X,y,0.1))\n",
    "print(precision_recall(X,y,0.2))\n",
    "print(precision_recall(X,y,0.3))\n",
    "print(precision_recall(X,y,0.4))\n",
    "print(precision_recall(X,y,0.5))\n",
    "\n",
    "# Please go and plot the Precision/Recall curve\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(476, 0, 513, 11)\n"
     ]
    }
   ],
   "source": [
    "# Useful function \n",
    "\n",
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "           TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "           FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "           TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "           FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "print(perf_measure(actual, predicted))\n",
    "\n",
    "# Use this function to now plot an ROC for the same problem\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
